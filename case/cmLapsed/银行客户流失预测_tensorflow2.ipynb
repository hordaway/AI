{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/select-data.csv\")\n",
    "test_df = pd.read_csv(\"./data/scalar-test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据来源于国外匿名化处理后的真实数据\n",
    "\n",
    "RowNumber：行号  \n",
    "CustomerID：用户编号  \n",
    "Surname：用户姓名  \n",
    "CreditScore：信用分数  \n",
    "Geography：用户所在国家/地区  \n",
    "Gender：用户性别  \n",
    "Age：年龄  \n",
    "Tenure：当了本银行多少年用户  \n",
    "Balance：存贷款情况  \n",
    "NumOfProducts：使用产品数量  \n",
    "HasCrCard：是否有本行信用卡  \n",
    "IsActiveMember：是否活跃用户  \n",
    "EstimatedSalary：估计收入  \n",
    "Exited：是否已流失，这将作为我们的标签数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "离散型变量都需要进行onehot编码  \n",
    "Gender/Geography/HasCrCard/IsActiveMember \n",
    "Age/CreditScore/EB/EstimatedSalary/NumOfProducts/Tenure 已经做过标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>EB</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Geography</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506735</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.310811</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.562709</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.569654</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.283784</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469120</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.337838</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Age  CreditScore        EB  EstimatedSalary  Gender  \\\n",
       "0           0  0.324324        0.538  0.000000         0.506735       0   \n",
       "1           1  0.310811        0.516  0.000070         0.562709       0   \n",
       "2           2  0.324324        0.304  0.000132         0.569654       0   \n",
       "3           3  0.283784        0.698  0.000000         0.469120       0   \n",
       "4           4  0.337838        1.000  0.000150         0.395400       0   \n",
       "\n",
       "   Geography  HasCrCard  IsActiveMember  NumOfProducts  Tenure  Exited  \n",
       "0        0.0          1               1       0.000000     0.2       1  \n",
       "1        0.5          0               1       0.000000     0.1       0  \n",
       "2        0.0          1               0       0.666667     0.8       1  \n",
       "3        0.0          0               0       0.333333     0.1       0  \n",
       "4        0.5          1               1       0.000000     0.2       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>EB</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.101298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411157</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419421</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.644939</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.626033</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.406481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.939612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Geography  Gender        EB       Age  EstimatedSalary  \\\n",
       "0           0        0.0     1.0  0.000000  0.178082         0.101298   \n",
       "1           1        0.0     1.0  0.000000  0.219178         0.019060   \n",
       "2           2        0.0     0.0  0.000000  0.027397         0.644939   \n",
       "3           3        0.0     1.0  0.004121  0.287671         0.406481   \n",
       "4           4        0.5     1.0  0.001127  0.136986         0.939612   \n",
       "\n",
       "   NumOfProducts  CreditScore  Tenure  HasCrCard  IsActiveMember  Exited  \n",
       "0       0.000000     0.411157     0.1        0.0             1.0     0.0  \n",
       "1       0.000000     0.419421     0.4        0.0             1.0     0.0  \n",
       "2       0.333333     0.626033     0.7        1.0             0.0     0.0  \n",
       "3       0.000000     0.677686     0.4        1.0             1.0     0.0  \n",
       "4       0.000000     0.285124     1.0        1.0             1.0     0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.drop(train_df.columns[[0]],axis=1,inplace=False)\n",
    "test_data = test_df.drop(train_df.columns[[0]],axis=1,inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>EB</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Geography</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506735</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.310811</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.562709</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.569654</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.283784</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469120</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.337838</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  CreditScore        EB  EstimatedSalary  Gender  Geography  \\\n",
       "0  0.324324        0.538  0.000000         0.506735       0        0.0   \n",
       "1  0.310811        0.516  0.000070         0.562709       0        0.5   \n",
       "2  0.324324        0.304  0.000132         0.569654       0        0.0   \n",
       "3  0.283784        0.698  0.000000         0.469120       0        0.0   \n",
       "4  0.337838        1.000  0.000150         0.395400       0        0.5   \n",
       "\n",
       "   HasCrCard  IsActiveMember  NumOfProducts  Tenure  Exited  \n",
       "0          1               1       0.000000     0.2       1  \n",
       "1          0               1       0.000000     0.1       0  \n",
       "2          1               0       0.666667     0.8       1  \n",
       "3          0               0       0.333333     0.1       0  \n",
       "4          1               1       0.000000     0.2       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>EB</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.101298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411157</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.419421</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.644939</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.626033</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.406481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.939612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Geography  Gender        EB       Age  EstimatedSalary  NumOfProducts  \\\n",
       "0        0.0     1.0  0.000000  0.178082         0.101298       0.000000   \n",
       "1        0.0     1.0  0.000000  0.219178         0.019060       0.000000   \n",
       "2        0.0     0.0  0.000000  0.027397         0.644939       0.333333   \n",
       "3        0.0     1.0  0.004121  0.287671         0.406481       0.000000   \n",
       "4        0.5     1.0  0.001127  0.136986         0.939612       0.000000   \n",
       "\n",
       "   CreditScore  Tenure  HasCrCard  IsActiveMember  Exited  \n",
       "0     0.411157     0.1        0.0             1.0     0.0  \n",
       "1     0.419421     0.4        0.0             1.0     0.0  \n",
       "2     0.626033     0.7        1.0             0.0     0.0  \n",
       "3     0.677686     0.4        1.0             1.0     0.0  \n",
       "4     0.285124     1.0        1.0             1.0     0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8149 entries, 0 to 8148\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Age              8149 non-null   float64\n",
      " 1   CreditScore      8149 non-null   float64\n",
      " 2   EB               8149 non-null   float64\n",
      " 3   EstimatedSalary  8149 non-null   float64\n",
      " 4   Gender           8149 non-null   int64  \n",
      " 5   Geography        8149 non-null   float64\n",
      " 6   HasCrCard        8149 non-null   int64  \n",
      " 7   IsActiveMember   8149 non-null   int64  \n",
      " 8   NumOfProducts    8149 non-null   float64\n",
      " 9   Tenure           8149 non-null   float64\n",
      " 10  Exited           8149 non-null   int64  \n",
      "dtypes: float64(7), int64(4)\n",
      "memory usage: 700.4 KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>EB</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>Tenure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.411157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.419421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.626033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644939</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.406481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.285124</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.939612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.131640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.106347</td>\n",
       "      <td>0.011246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.638430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.812970</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945530</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  CreditScore        EB  EstimatedSalary  NumOfProducts  Tenure\n",
       "0    0.178082     0.411157  0.000000         0.101298       0.000000     0.1\n",
       "1    0.219178     0.419421  0.000000         0.019060       0.000000     0.4\n",
       "2    0.027397     0.626033  0.000000         0.644939       0.333333     0.7\n",
       "3    0.287671     0.677686  0.004121         0.406481       0.000000     0.4\n",
       "4    0.136986     0.285124  0.001127         0.939612       0.000000     1.0\n",
       "..        ...          ...       ...              ...            ...     ...\n",
       "995  0.219178     0.340909  0.008635         0.131640       0.000000     1.0\n",
       "996  0.424658     0.431818  0.106347         0.011246       0.000000     0.2\n",
       "997  0.767123     0.318182  0.000000         0.132894       0.000000     0.4\n",
       "998  0.068493     0.638430  0.000000         0.812970       0.333333     0.8\n",
       "999  0.383562     0.657025  0.000000         0.945530       0.333333     1.0\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cColumnList = train_data[['Age','CreditScore','EB','EstimatedSalary','NumOfProducts','Tenure']]\n",
    "cColumnList\n",
    "\n",
    "cColumnListTest = test_data[['Age','CreditScore','EB','EstimatedSalary','NumOfProducts','Tenure']]\n",
    "cColumnListTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8\n",
       "0    0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0\n",
       "1    0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0\n",
       "2    1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0\n",
       "3    0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0\n",
       "4    0.0  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0\n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "995  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0\n",
       "996  0.0  1.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0\n",
       "997  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0\n",
       "998  0.0  1.0  0.0  1.0  0.0  1.0  0.0  1.0  0.0\n",
       "999  0.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0\n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHotColumnList = pd.DataFrame(OneHotEncoder().fit_transform(\n",
    "                train_data[['Gender','Geography','HasCrCard','IsActiveMember']]).toarray()\n",
    "                )\n",
    "oneHotColumnList\n",
    "\n",
    "\n",
    "\n",
    "oneHotColumnListTest = pd.DataFrame(OneHotEncoder().fit_transform(\n",
    "                test_data[['Gender','Geography','HasCrCard','IsActiveMember']]).toarray()\n",
    "                )\n",
    "oneHotColumnListTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>EB</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.411157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.419421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.626033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644939</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.287671</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.406481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.285124</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.939612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.008635</td>\n",
       "      <td>0.131640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.424658</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.106347</td>\n",
       "      <td>0.011246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.068493</td>\n",
       "      <td>0.638430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.812970</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.657025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945530</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  CreditScore        EB  EstimatedSalary  NumOfProducts  Tenure  \\\n",
       "0    0.178082     0.411157  0.000000         0.101298       0.000000     0.1   \n",
       "1    0.219178     0.419421  0.000000         0.019060       0.000000     0.4   \n",
       "2    0.027397     0.626033  0.000000         0.644939       0.333333     0.7   \n",
       "3    0.287671     0.677686  0.004121         0.406481       0.000000     0.4   \n",
       "4    0.136986     0.285124  0.001127         0.939612       0.000000     1.0   \n",
       "..        ...          ...       ...              ...            ...     ...   \n",
       "995  0.219178     0.340909  0.008635         0.131640       0.000000     1.0   \n",
       "996  0.424658     0.431818  0.106347         0.011246       0.000000     0.2   \n",
       "997  0.767123     0.318182  0.000000         0.132894       0.000000     0.4   \n",
       "998  0.068493     0.638430  0.000000         0.812970       0.333333     0.8   \n",
       "999  0.383562     0.657025  0.000000         0.945530       0.333333     1.0   \n",
       "\n",
       "       0    1    2    3    4    5    6    7    8  \n",
       "0    0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  \n",
       "1    0.0  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  \n",
       "2    1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  \n",
       "3    0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  1.0  \n",
       "4    0.0  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "995  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  \n",
       "996  0.0  1.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0  \n",
       "997  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  \n",
       "998  0.0  1.0  0.0  1.0  0.0  1.0  0.0  1.0  0.0  \n",
       "999  0.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  \n",
       "\n",
       "[1000 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX = pd.concat([cColumnList, oneHotColumnList], axis=1) \n",
    "dataX\n",
    "\n",
    "dataXTest = pd.concat([cColumnListTest, oneHotColumnListTest], axis=1) \n",
    "dataXTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dataX\n",
    "x_train\n",
    "y_train = train_data[['Exited']]\n",
    "y_train\n",
    "x_train,x_valid,y_train,y_valid= train_test_split(x_train,y_train,test_size=0.2)\n",
    "x_test = dataXTest\n",
    "y_test = test_data[['Exited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping=tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                                min_delta=0.0001,\n",
    "                                                patience=50, \n",
    "                                                mode='auto',\n",
    "                                                baseline=None, \n",
    "                                                restore_best_weights=False,\n",
    "                                                verbose=1)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
    "                                                 patience=1,\n",
    "                                                 verbose=1,\n",
    "                                                 factor = 0.1)\n",
    "\n",
    "checkpoint_save_path = \"./checkpoint/cmLapsed.ckpt\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
    "                                                monitor='loss', \n",
    "                                                save_weights_only=True,\n",
    "                                                verbose=1,\n",
    "                                                save_best_only=True,\n",
    "                                                save_freq='epoch',\n",
    "                                                mode = 'min',\n",
    "                                                max_to_keep=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.6450 - accuracy: 0.6184\n",
      "Epoch 00001: loss improved from inf to 0.63865, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.6386 - accuracy: 0.6286 - val_loss: 0.6167 - val_accuracy: 0.6699\n",
      "Epoch 2/500\n",
      "33/51 [==================>...........] - ETA: 0s - loss: 0.6009 - accuracy: 0.6854\n",
      "Epoch 00002: loss improved from 0.63865 to 0.59679, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5968 - accuracy: 0.6835 - val_loss: 0.5846 - val_accuracy: 0.6804\n",
      "Epoch 3/500\n",
      "33/51 [==================>...........] - ETA: 0s - loss: 0.5833 - accuracy: 0.6877\n",
      "Epoch 00003: loss improved from 0.59679 to 0.57605, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5760 - accuracy: 0.6972 - val_loss: 0.5658 - val_accuracy: 0.6988\n",
      "Epoch 4/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.5571 - accuracy: 0.7212\n",
      "Epoch 00004: loss improved from 0.57605 to 0.55095, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5509 - accuracy: 0.7219 - val_loss: 0.5340 - val_accuracy: 0.7276\n",
      "Epoch 5/500\n",
      "33/51 [==================>...........] - ETA: 0s - loss: 0.5321 - accuracy: 0.7372\n",
      "Epoch 00005: loss improved from 0.55095 to 0.52729, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.7385 - val_loss: 0.5173 - val_accuracy: 0.7479\n",
      "Epoch 6/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.5189 - accuracy: 0.7432\n",
      "Epoch 00006: loss improved from 0.52729 to 0.50978, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5098 - accuracy: 0.7504 - val_loss: 0.5054 - val_accuracy: 0.7571\n",
      "Epoch 7/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.5044 - accuracy: 0.7581\n",
      "Epoch 00007: loss improved from 0.50978 to 0.50350, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.5035 - accuracy: 0.7546 - val_loss: 0.4969 - val_accuracy: 0.7583\n",
      "Epoch 8/500\n",
      "33/51 [==================>...........] - ETA: 0s - loss: 0.4896 - accuracy: 0.7562\n",
      "Epoch 00008: loss improved from 0.50350 to 0.49269, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4927 - accuracy: 0.7590 - val_loss: 0.4967 - val_accuracy: 0.7497\n",
      "Epoch 9/500\n",
      "47/51 [==========================>...] - ETA: 0s - loss: 0.4923 - accuracy: 0.7621\n",
      "Epoch 00009: loss did not improve from 0.49269\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4927 - accuracy: 0.7618 - val_loss: 0.4917 - val_accuracy: 0.7497\n",
      "Epoch 10/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.4898 - accuracy: 0.7648\n",
      "Epoch 00010: loss improved from 0.49269 to 0.49021, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.7632 - val_loss: 0.4919 - val_accuracy: 0.7589\n",
      "Epoch 11/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4862 - accuracy: 0.7618\n",
      "Epoch 00011: loss improved from 0.49021 to 0.48655, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4866 - accuracy: 0.7633 - val_loss: 0.4987 - val_accuracy: 0.7454\n",
      "Epoch 12/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4773 - accuracy: 0.7737\n",
      "Epoch 00012: loss improved from 0.48655 to 0.47905, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7688 - val_loss: 0.4872 - val_accuracy: 0.7534\n",
      "Epoch 13/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4756 - accuracy: 0.7747\n",
      "Epoch 00013: loss improved from 0.47905 to 0.47902, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4790 - accuracy: 0.7727 - val_loss: 0.4884 - val_accuracy: 0.7577\n",
      "Epoch 14/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4749 - accuracy: 0.7699\n",
      "Epoch 00014: loss improved from 0.47902 to 0.47805, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4781 - accuracy: 0.7650 - val_loss: 0.4875 - val_accuracy: 0.7571\n",
      "Epoch 15/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4790 - accuracy: 0.7742\n",
      "Epoch 00015: loss improved from 0.47805 to 0.47615, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4762 - accuracy: 0.7719 - val_loss: 0.4832 - val_accuracy: 0.7626\n",
      "Epoch 16/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4752 - accuracy: 0.7685\n",
      "Epoch 00016: loss improved from 0.47615 to 0.47514, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4751 - accuracy: 0.7707 - val_loss: 0.4868 - val_accuracy: 0.7613\n",
      "Epoch 17/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.4732 - accuracy: 0.7676\n",
      "Epoch 00017: loss improved from 0.47514 to 0.47491, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4749 - accuracy: 0.7704 - val_loss: 0.4840 - val_accuracy: 0.7675\n",
      "Epoch 18/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4602 - accuracy: 0.7844\n",
      "Epoch 00018: loss improved from 0.47491 to 0.47076, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4708 - accuracy: 0.7773 - val_loss: 0.4795 - val_accuracy: 0.7644\n",
      "Epoch 19/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4582 - accuracy: 0.7930\n",
      "Epoch 00019: loss improved from 0.47076 to 0.46589, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4659 - accuracy: 0.7845 - val_loss: 0.4811 - val_accuracy: 0.7687\n",
      "Epoch 20/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4628 - accuracy: 0.7820\n",
      "Epoch 00020: loss did not improve from 0.46589\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.7754 - val_loss: 0.4853 - val_accuracy: 0.7718\n",
      "Epoch 21/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4661 - accuracy: 0.7766\n",
      "Epoch 00021: loss did not improve from 0.46589\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.7767 - val_loss: 0.4755 - val_accuracy: 0.7644\n",
      "Epoch 22/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4628 - accuracy: 0.7724\n",
      "Epoch 00022: loss improved from 0.46589 to 0.46216, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.7786 - val_loss: 0.4835 - val_accuracy: 0.7656\n",
      "Epoch 23/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4530 - accuracy: 0.7878\n",
      "Epoch 00023: loss did not improve from 0.46216\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4622 - accuracy: 0.7779 - val_loss: 0.4809 - val_accuracy: 0.7626\n",
      "Epoch 24/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4581 - accuracy: 0.7775\n",
      "Epoch 00024: loss improved from 0.46216 to 0.46088, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.7782 - val_loss: 0.4760 - val_accuracy: 0.7656\n",
      "Epoch 25/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/51 [================>.............] - ETA: 0s - loss: 0.4591 - accuracy: 0.7763\n",
      "Epoch 00025: loss did not improve from 0.46088\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7782 - val_loss: 0.4854 - val_accuracy: 0.7699\n",
      "Epoch 26/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4619 - accuracy: 0.7797\n",
      "Epoch 00026: loss improved from 0.46088 to 0.46027, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.7806 - val_loss: 0.4751 - val_accuracy: 0.7767\n",
      "Epoch 27/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4489 - accuracy: 0.7810\n",
      "Epoch 00027: loss improved from 0.46027 to 0.45747, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4575 - accuracy: 0.7813 - val_loss: 0.4764 - val_accuracy: 0.7669\n",
      "Epoch 28/500\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.4572 - accuracy: 0.7750\n",
      "Epoch 00028: loss improved from 0.45747 to 0.45627, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4563 - accuracy: 0.7754 - val_loss: 0.4753 - val_accuracy: 0.7669\n",
      "Epoch 29/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.4537 - accuracy: 0.7839\n",
      "Epoch 00029: loss did not improve from 0.45627\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4574 - accuracy: 0.7777 - val_loss: 0.4738 - val_accuracy: 0.7681\n",
      "Epoch 30/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4530 - accuracy: 0.7810\n",
      "Epoch 00030: loss improved from 0.45627 to 0.44859, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.7862 - val_loss: 0.4845 - val_accuracy: 0.7663\n",
      "Epoch 31/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4564 - accuracy: 0.7835\n",
      "Epoch 00031: loss did not improve from 0.44859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.7845 - val_loss: 0.4743 - val_accuracy: 0.7638\n",
      "Epoch 32/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4493 - accuracy: 0.7807\n",
      "Epoch 00032: loss did not improve from 0.44859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.7814 - val_loss: 0.4724 - val_accuracy: 0.7736\n",
      "Epoch 33/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4440 - accuracy: 0.7930\n",
      "Epoch 00033: loss did not improve from 0.44859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.7875 - val_loss: 0.4713 - val_accuracy: 0.7773\n",
      "Epoch 34/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4482 - accuracy: 0.7857\n",
      "Epoch 00034: loss did not improve from 0.44859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4486 - accuracy: 0.7877 - val_loss: 0.4711 - val_accuracy: 0.7712\n",
      "Epoch 35/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4530 - accuracy: 0.7916\n",
      "Epoch 00035: loss did not improve from 0.44859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.7937 - val_loss: 0.4729 - val_accuracy: 0.7669\n",
      "Epoch 36/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4435 - accuracy: 0.7853\n",
      "Epoch 00036: loss improved from 0.44859 to 0.44786, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4479 - accuracy: 0.7865 - val_loss: 0.4758 - val_accuracy: 0.7632\n",
      "Epoch 37/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4401 - accuracy: 0.7883\n",
      "Epoch 00037: loss improved from 0.44786 to 0.44209, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.7871 - val_loss: 0.4726 - val_accuracy: 0.7748\n",
      "Epoch 38/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4511 - accuracy: 0.7840\n",
      "Epoch 00038: loss did not improve from 0.44209\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.7845 - val_loss: 0.4711 - val_accuracy: 0.7644\n",
      "Epoch 39/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4381 - accuracy: 0.7885\n",
      "Epoch 00039: loss did not improve from 0.44209\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4435 - accuracy: 0.7902 - val_loss: 0.4710 - val_accuracy: 0.7718\n",
      "Epoch 40/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4377 - accuracy: 0.7933\n",
      "Epoch 00040: loss did not improve from 0.44209\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.7891 - val_loss: 0.4698 - val_accuracy: 0.7669\n",
      "Epoch 41/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4325 - accuracy: 0.7911\n",
      "Epoch 00041: loss improved from 0.44209 to 0.43804, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4380 - accuracy: 0.7880 - val_loss: 0.4685 - val_accuracy: 0.7675\n",
      "Epoch 42/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4456 - accuracy: 0.7803\n",
      "Epoch 00042: loss did not improve from 0.43804\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.7852 - val_loss: 0.4707 - val_accuracy: 0.7644\n",
      "Epoch 43/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.4441 - accuracy: 0.7927\n",
      "Epoch 00043: loss improved from 0.43804 to 0.43641, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.7944 - val_loss: 0.4677 - val_accuracy: 0.7669\n",
      "Epoch 44/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4427 - accuracy: 0.7880\n",
      "Epoch 00044: loss did not improve from 0.43641\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.7889 - val_loss: 0.4677 - val_accuracy: 0.7632\n",
      "Epoch 45/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.4202 - accuracy: 0.7969\n",
      "Epoch 00045: loss improved from 0.43641 to 0.43433, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.7880 - val_loss: 0.4672 - val_accuracy: 0.7620\n",
      "Epoch 46/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4337 - accuracy: 0.7966\n",
      "Epoch 00046: loss did not improve from 0.43433\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.7906 - val_loss: 0.4680 - val_accuracy: 0.7742\n",
      "Epoch 47/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4377 - accuracy: 0.7966\n",
      "Epoch 00047: loss did not improve from 0.43433\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.7928 - val_loss: 0.4698 - val_accuracy: 0.7632\n",
      "Epoch 48/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4334 - accuracy: 0.7949\n",
      "Epoch 00048: loss did not improve from 0.43433\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7905 - val_loss: 0.4648 - val_accuracy: 0.7644\n",
      "Epoch 49/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4281 - accuracy: 0.7954\n",
      "Epoch 00049: loss improved from 0.43433 to 0.43113, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.7940 - val_loss: 0.4690 - val_accuracy: 0.7663\n",
      "Epoch 50/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4328 - accuracy: 0.7932\n",
      "Epoch 00050: loss did not improve from 0.43113\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.7934 - val_loss: 0.4662 - val_accuracy: 0.7736\n",
      "Epoch 51/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4198 - accuracy: 0.8042\n",
      "Epoch 00051: loss did not improve from 0.43113\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.7966 - val_loss: 0.4661 - val_accuracy: 0.7730\n",
      "Epoch 52/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4382 - accuracy: 0.7797\n",
      "Epoch 00052: loss did not improve from 0.43113\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.7859 - val_loss: 0.4679 - val_accuracy: 0.7706\n",
      "Epoch 53/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4242 - accuracy: 0.7949\n",
      "Epoch 00053: loss improved from 0.43113 to 0.42959, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4296 - accuracy: 0.7954 - val_loss: 0.4639 - val_accuracy: 0.7681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4262 - accuracy: 0.7966\n",
      "Epoch 00054: loss improved from 0.42959 to 0.42650, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4265 - accuracy: 0.7984 - val_loss: 0.4635 - val_accuracy: 0.7706\n",
      "Epoch 55/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4270 - accuracy: 0.7994\n",
      "Epoch 00055: loss did not improve from 0.42650\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.7971 - val_loss: 0.4691 - val_accuracy: 0.7620\n",
      "Epoch 56/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4245 - accuracy: 0.8003\n",
      "Epoch 00056: loss did not improve from 0.42650\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4302 - accuracy: 0.7948 - val_loss: 0.4614 - val_accuracy: 0.7681\n",
      "Epoch 57/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4279 - accuracy: 0.8010\n",
      "Epoch 00057: loss improved from 0.42650 to 0.42625, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.7992 - val_loss: 0.4692 - val_accuracy: 0.7718\n",
      "Epoch 58/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4167 - accuracy: 0.8066\n",
      "Epoch 00058: loss improved from 0.42625 to 0.42308, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4231 - accuracy: 0.7969 - val_loss: 0.4632 - val_accuracy: 0.7699\n",
      "Epoch 59/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.4237 - accuracy: 0.8003\n",
      "Epoch 00059: loss improved from 0.42308 to 0.42286, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.7994 - val_loss: 0.4662 - val_accuracy: 0.7681\n",
      "Epoch 60/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4295 - accuracy: 0.7986\n",
      "Epoch 00060: loss did not improve from 0.42286\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4256 - accuracy: 0.7977 - val_loss: 0.4670 - val_accuracy: 0.7748\n",
      "Epoch 61/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4189 - accuracy: 0.7981\n",
      "Epoch 00061: loss improved from 0.42286 to 0.42186, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4219 - accuracy: 0.7989 - val_loss: 0.4652 - val_accuracy: 0.7712\n",
      "Epoch 62/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4204 - accuracy: 0.7997\n",
      "Epoch 00062: loss did not improve from 0.42186\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.8003 - val_loss: 0.4658 - val_accuracy: 0.7706\n",
      "Epoch 63/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4214 - accuracy: 0.8062\n",
      "Epoch 00063: loss did not improve from 0.42186\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4250 - accuracy: 0.8033 - val_loss: 0.4673 - val_accuracy: 0.7632\n",
      "Epoch 64/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4264 - accuracy: 0.7979\n",
      "Epoch 00064: loss did not improve from 0.42186\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8001 - val_loss: 0.4657 - val_accuracy: 0.7644\n",
      "Epoch 65/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4242 - accuracy: 0.8044\n",
      "Epoch 00065: loss improved from 0.42186 to 0.42076, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4208 - accuracy: 0.8032 - val_loss: 0.4644 - val_accuracy: 0.7638\n",
      "Epoch 66/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4110 - accuracy: 0.8047\n",
      "Epoch 00066: loss improved from 0.42076 to 0.41515, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4152 - accuracy: 0.8020 - val_loss: 0.4643 - val_accuracy: 0.7712\n",
      "Epoch 67/500\n",
      "47/51 [==========================>...] - ETA: 0s - loss: 0.4153 - accuracy: 0.8020\n",
      "Epoch 00067: loss did not improve from 0.41515\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4158 - accuracy: 0.8024 - val_loss: 0.4624 - val_accuracy: 0.7718\n",
      "Epoch 68/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4195 - accuracy: 0.8003\n",
      "Epoch 00068: loss did not improve from 0.41515\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8021 - val_loss: 0.4597 - val_accuracy: 0.7724\n",
      "Epoch 69/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.4119 - accuracy: 0.8042\n",
      "Epoch 00069: loss improved from 0.41515 to 0.41217, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4122 - accuracy: 0.8087 - val_loss: 0.4628 - val_accuracy: 0.7748\n",
      "Epoch 70/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4154 - accuracy: 0.7961\n",
      "Epoch 00070: loss did not improve from 0.41217\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4145 - accuracy: 0.7994 - val_loss: 0.4613 - val_accuracy: 0.7742\n",
      "Epoch 71/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4088 - accuracy: 0.8037\n",
      "Epoch 00071: loss did not improve from 0.41217\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8010 - val_loss: 0.4620 - val_accuracy: 0.7687\n",
      "Epoch 72/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3979 - accuracy: 0.8130\n",
      "Epoch 00072: loss improved from 0.41217 to 0.40930, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4093 - accuracy: 0.8052 - val_loss: 0.4580 - val_accuracy: 0.7767\n",
      "Epoch 73/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3949 - accuracy: 0.8070\n",
      "Epoch 00073: loss improved from 0.40930 to 0.40636, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4064 - accuracy: 0.8026 - val_loss: 0.4562 - val_accuracy: 0.7718\n",
      "Epoch 74/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4032 - accuracy: 0.8117\n",
      "Epoch 00074: loss did not improve from 0.40636\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8027 - val_loss: 0.4585 - val_accuracy: 0.7712\n",
      "Epoch 75/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4046 - accuracy: 0.8095\n",
      "Epoch 00075: loss did not improve from 0.40636\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8066 - val_loss: 0.4618 - val_accuracy: 0.7693\n",
      "Epoch 76/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4037 - accuracy: 0.8037\n",
      "Epoch 00076: loss did not improve from 0.40636\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4118 - accuracy: 0.8020 - val_loss: 0.4621 - val_accuracy: 0.7742\n",
      "Epoch 77/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4034 - accuracy: 0.8125\n",
      "Epoch 00077: loss improved from 0.40636 to 0.40444, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.4044 - accuracy: 0.8099 - val_loss: 0.4638 - val_accuracy: 0.7650\n",
      "Epoch 78/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4021 - accuracy: 0.8067\n",
      "Epoch 00078: loss did not improve from 0.40444\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8060 - val_loss: 0.4614 - val_accuracy: 0.7607\n",
      "Epoch 79/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3863 - accuracy: 0.8208\n",
      "Epoch 00079: loss did not improve from 0.40444\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8089 - val_loss: 0.4571 - val_accuracy: 0.7742\n",
      "Epoch 80/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4022 - accuracy: 0.8082\n",
      "Epoch 00080: loss improved from 0.40444 to 0.39889, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3989 - accuracy: 0.8093 - val_loss: 0.4625 - val_accuracy: 0.7742\n",
      "Epoch 81/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.4144 - accuracy: 0.8039\n",
      "Epoch 00081: loss did not improve from 0.39889\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8030 - val_loss: 0.4552 - val_accuracy: 0.7853\n",
      "Epoch 82/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/51 [=================>............] - ETA: 0s - loss: 0.4055 - accuracy: 0.8030\n",
      "Epoch 00082: loss did not improve from 0.39889\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8033 - val_loss: 0.4548 - val_accuracy: 0.7681\n",
      "Epoch 83/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.4117 - accuracy: 0.7998\n",
      "Epoch 00083: loss did not improve from 0.39889\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.4020 - accuracy: 0.8083 - val_loss: 0.4574 - val_accuracy: 0.7761\n",
      "Epoch 84/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.4010 - accuracy: 0.8130\n",
      "Epoch 00084: loss improved from 0.39889 to 0.39364, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3936 - accuracy: 0.8176 - val_loss: 0.4599 - val_accuracy: 0.7742\n",
      "Epoch 85/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3874 - accuracy: 0.8191\n",
      "Epoch 00085: loss did not improve from 0.39364\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8136 - val_loss: 0.4611 - val_accuracy: 0.7589\n",
      "Epoch 86/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3853 - accuracy: 0.8217\n",
      "Epoch 00086: loss did not improve from 0.39364\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8113 - val_loss: 0.4597 - val_accuracy: 0.7706\n",
      "Epoch 87/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3901 - accuracy: 0.8206\n",
      "Epoch 00087: loss did not improve from 0.39364\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8104 - val_loss: 0.4546 - val_accuracy: 0.7706\n",
      "Epoch 88/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.3857 - accuracy: 0.8206\n",
      "Epoch 00088: loss improved from 0.39364 to 0.39304, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3930 - accuracy: 0.8161 - val_loss: 0.4557 - val_accuracy: 0.7730\n",
      "Epoch 89/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3964 - accuracy: 0.8102\n",
      "Epoch 00089: loss did not improve from 0.39304\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8102 - val_loss: 0.4511 - val_accuracy: 0.7785\n",
      "Epoch 90/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3829 - accuracy: 0.8130\n",
      "Epoch 00090: loss improved from 0.39304 to 0.39165, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8124 - val_loss: 0.4532 - val_accuracy: 0.7810\n",
      "Epoch 91/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3891 - accuracy: 0.8179\n",
      "Epoch 00091: loss did not improve from 0.39165\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8113 - val_loss: 0.4530 - val_accuracy: 0.7810\n",
      "Epoch 92/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3817 - accuracy: 0.8262\n",
      "Epoch 00092: loss improved from 0.39165 to 0.39058, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 0.8171 - val_loss: 0.4525 - val_accuracy: 0.7687\n",
      "Epoch 93/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3865 - accuracy: 0.8178\n",
      "Epoch 00093: loss did not improve from 0.39058\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8156 - val_loss: 0.4567 - val_accuracy: 0.7730\n",
      "Epoch 94/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3797 - accuracy: 0.8225\n",
      "Epoch 00094: loss improved from 0.39058 to 0.38983, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8148 - val_loss: 0.4549 - val_accuracy: 0.7712\n",
      "Epoch 95/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3817 - accuracy: 0.8179\n",
      "Epoch 00095: loss improved from 0.38983 to 0.38856, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3886 - accuracy: 0.8164 - val_loss: 0.4555 - val_accuracy: 0.7718\n",
      "Epoch 96/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3996 - accuracy: 0.8110\n",
      "Epoch 00096: loss did not improve from 0.38856\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8161 - val_loss: 0.4582 - val_accuracy: 0.7742\n",
      "Epoch 97/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3885 - accuracy: 0.8187\n",
      "Epoch 00097: loss improved from 0.38856 to 0.38446, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3845 - accuracy: 0.8214 - val_loss: 0.4573 - val_accuracy: 0.7742\n",
      "Epoch 98/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3788 - accuracy: 0.8193\n",
      "Epoch 00098: loss improved from 0.38446 to 0.38134, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3813 - accuracy: 0.8196 - val_loss: 0.4582 - val_accuracy: 0.7724\n",
      "Epoch 99/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3826 - accuracy: 0.8219\n",
      "Epoch 00099: loss did not improve from 0.38134\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8236 - val_loss: 0.4522 - val_accuracy: 0.7785\n",
      "Epoch 100/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.3793 - accuracy: 0.8211\n",
      "Epoch 00100: loss improved from 0.38134 to 0.38125, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8224 - val_loss: 0.4631 - val_accuracy: 0.7755\n",
      "Epoch 101/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3803 - accuracy: 0.8163\n",
      "Epoch 00101: loss did not improve from 0.38125\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8150 - val_loss: 0.4558 - val_accuracy: 0.7718\n",
      "Epoch 102/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3773 - accuracy: 0.8195\n",
      "Epoch 00102: loss improved from 0.38125 to 0.38083, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3808 - accuracy: 0.8202 - val_loss: 0.4511 - val_accuracy: 0.7718\n",
      "Epoch 103/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3783 - accuracy: 0.8228\n",
      "Epoch 00103: loss did not improve from 0.38083\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8199 - val_loss: 0.4561 - val_accuracy: 0.7816\n",
      "Epoch 104/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3700 - accuracy: 0.8250\n",
      "Epoch 00104: loss did not improve from 0.38083\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8195 - val_loss: 0.4523 - val_accuracy: 0.7773\n",
      "Epoch 105/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3797 - accuracy: 0.8241\n",
      "Epoch 00105: loss improved from 0.38083 to 0.37901, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8221 - val_loss: 0.4535 - val_accuracy: 0.7871\n",
      "Epoch 106/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3779 - accuracy: 0.8255\n",
      "Epoch 00106: loss did not improve from 0.37901\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8237 - val_loss: 0.4535 - val_accuracy: 0.7785\n",
      "Epoch 107/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3695 - accuracy: 0.8240\n",
      "Epoch 00107: loss did not improve from 0.37901\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3793 - accuracy: 0.8185 - val_loss: 0.4539 - val_accuracy: 0.7742\n",
      "Epoch 108/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3799 - accuracy: 0.8289\n",
      "Epoch 00108: loss did not improve from 0.37901\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8213 - val_loss: 0.4522 - val_accuracy: 0.7767\n",
      "Epoch 109/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3854 - accuracy: 0.8142\n",
      "Epoch 00109: loss improved from 0.37901 to 0.37765, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3777 - accuracy: 0.8190 - val_loss: 0.4526 - val_accuracy: 0.7804\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/51 [================>.............] - ETA: 0s - loss: 0.3715 - accuracy: 0.8258\n",
      "Epoch 00110: loss improved from 0.37765 to 0.37502, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3750 - accuracy: 0.8213 - val_loss: 0.4499 - val_accuracy: 0.7828\n",
      "Epoch 111/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3754 - accuracy: 0.8220\n",
      "Epoch 00111: loss did not improve from 0.37502\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8237 - val_loss: 0.4475 - val_accuracy: 0.7816\n",
      "Epoch 112/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3768 - accuracy: 0.8256\n",
      "Epoch 00112: loss improved from 0.37502 to 0.37363, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8262 - val_loss: 0.4533 - val_accuracy: 0.7767\n",
      "Epoch 113/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3702 - accuracy: 0.8246\n",
      "Epoch 00113: loss did not improve from 0.37363\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8260 - val_loss: 0.4542 - val_accuracy: 0.7706\n",
      "Epoch 114/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3737 - accuracy: 0.8214\n",
      "Epoch 00114: loss improved from 0.37363 to 0.36915, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3692 - accuracy: 0.8244 - val_loss: 0.4516 - val_accuracy: 0.7822\n",
      "Epoch 115/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3669 - accuracy: 0.8262\n",
      "Epoch 00115: loss did not improve from 0.36915\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3719 - accuracy: 0.8208 - val_loss: 0.4549 - val_accuracy: 0.7693\n",
      "Epoch 116/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3706 - accuracy: 0.8228\n",
      "Epoch 00116: loss did not improve from 0.36915\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8244 - val_loss: 0.4485 - val_accuracy: 0.7840\n",
      "Epoch 117/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3540 - accuracy: 0.8296\n",
      "Epoch 00117: loss improved from 0.36915 to 0.36489, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8242 - val_loss: 0.4486 - val_accuracy: 0.7785\n",
      "Epoch 118/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3649 - accuracy: 0.8344\n",
      "Epoch 00118: loss did not improve from 0.36489\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8322 - val_loss: 0.4538 - val_accuracy: 0.7798\n",
      "Epoch 119/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3674 - accuracy: 0.8299\n",
      "Epoch 00119: loss did not improve from 0.36489\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8300 - val_loss: 0.4481 - val_accuracy: 0.7920\n",
      "Epoch 120/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3695 - accuracy: 0.8228\n",
      "Epoch 00120: loss did not improve from 0.36489\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8277 - val_loss: 0.4525 - val_accuracy: 0.7736\n",
      "Epoch 121/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3649 - accuracy: 0.8306\n",
      "Epoch 00121: loss did not improve from 0.36489\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.8300 - val_loss: 0.4528 - val_accuracy: 0.7779\n",
      "Epoch 122/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3642 - accuracy: 0.8306\n",
      "Epoch 00122: loss improved from 0.36489 to 0.36225, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.8329 - val_loss: 0.4531 - val_accuracy: 0.7767\n",
      "Epoch 123/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3663 - accuracy: 0.8301\n",
      "Epoch 00123: loss did not improve from 0.36225\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8287 - val_loss: 0.4577 - val_accuracy: 0.7834\n",
      "Epoch 124/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3712 - accuracy: 0.8268\n",
      "Epoch 00124: loss did not improve from 0.36225\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3669 - accuracy: 0.8297 - val_loss: 0.4522 - val_accuracy: 0.7816\n",
      "Epoch 125/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3672 - accuracy: 0.8304\n",
      "Epoch 00125: loss did not improve from 0.36225\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.8308 - val_loss: 0.4541 - val_accuracy: 0.7871\n",
      "Epoch 126/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3522 - accuracy: 0.8281\n",
      "Epoch 00126: loss improved from 0.36225 to 0.35924, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3592 - accuracy: 0.8291 - val_loss: 0.4518 - val_accuracy: 0.7834\n",
      "Epoch 127/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3601 - accuracy: 0.8292\n",
      "Epoch 00127: loss improved from 0.35924 to 0.35707, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3571 - accuracy: 0.8319 - val_loss: 0.4541 - val_accuracy: 0.7816\n",
      "Epoch 128/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3566 - accuracy: 0.8354\n",
      "Epoch 00128: loss improved from 0.35707 to 0.35477, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.8379 - val_loss: 0.4519 - val_accuracy: 0.7890\n",
      "Epoch 129/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3559 - accuracy: 0.8299\n",
      "Epoch 00129: loss did not improve from 0.35477\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3616 - accuracy: 0.8316 - val_loss: 0.4483 - val_accuracy: 0.7871\n",
      "Epoch 130/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3491 - accuracy: 0.8409\n",
      "Epoch 00130: loss did not improve from 0.35477\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.8329 - val_loss: 0.4476 - val_accuracy: 0.7871\n",
      "Epoch 131/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3527 - accuracy: 0.8347\n",
      "Epoch 00131: loss did not improve from 0.35477\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3555 - accuracy: 0.8333 - val_loss: 0.4457 - val_accuracy: 0.7896\n",
      "Epoch 132/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3603 - accuracy: 0.8291\n",
      "Epoch 00132: loss did not improve from 0.35477\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8328 - val_loss: 0.4466 - val_accuracy: 0.7883\n",
      "Epoch 133/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3574 - accuracy: 0.8325\n",
      "Epoch 00133: loss did not improve from 0.35477\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8333 - val_loss: 0.4447 - val_accuracy: 0.7748\n",
      "Epoch 134/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3508 - accuracy: 0.8306\n",
      "Epoch 00134: loss did not improve from 0.35477\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8283 - val_loss: 0.4474 - val_accuracy: 0.7871\n",
      "Epoch 135/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3461 - accuracy: 0.8373\n",
      "Epoch 00135: loss improved from 0.35477 to 0.35139, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3514 - accuracy: 0.8339 - val_loss: 0.4473 - val_accuracy: 0.7828\n",
      "Epoch 136/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3469 - accuracy: 0.8389\n",
      "Epoch 00136: loss improved from 0.35139 to 0.35047, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3505 - accuracy: 0.8359 - val_loss: 0.4547 - val_accuracy: 0.7859\n",
      "Epoch 137/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3523 - accuracy: 0.8372\n",
      "Epoch 00137: loss did not improve from 0.35047\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3553 - accuracy: 0.8336 - val_loss: 0.4551 - val_accuracy: 0.7804\n",
      "Epoch 138/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/51 [=================>............] - ETA: 0s - loss: 0.3447 - accuracy: 0.8345\n",
      "Epoch 00138: loss improved from 0.35047 to 0.34912, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3491 - accuracy: 0.8339 - val_loss: 0.4514 - val_accuracy: 0.7883\n",
      "Epoch 139/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3476 - accuracy: 0.8349\n",
      "Epoch 00139: loss improved from 0.34912 to 0.34808, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8379 - val_loss: 0.4473 - val_accuracy: 0.7853\n",
      "Epoch 140/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3398 - accuracy: 0.8443\n",
      "Epoch 00140: loss improved from 0.34808 to 0.34582, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3458 - accuracy: 0.8386 - val_loss: 0.4499 - val_accuracy: 0.7834\n",
      "Epoch 141/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3458 - accuracy: 0.8381\n",
      "Epoch 00141: loss improved from 0.34582 to 0.34418, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3442 - accuracy: 0.8382 - val_loss: 0.4483 - val_accuracy: 0.7902\n",
      "Epoch 142/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3439 - accuracy: 0.8412\n",
      "Epoch 00142: loss did not improve from 0.34418\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8395 - val_loss: 0.4485 - val_accuracy: 0.7798\n",
      "Epoch 143/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3424 - accuracy: 0.8396\n",
      "Epoch 00143: loss did not improve from 0.34418\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8382 - val_loss: 0.4521 - val_accuracy: 0.7847\n",
      "Epoch 144/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3449 - accuracy: 0.8417\n",
      "Epoch 00144: loss did not improve from 0.34418\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8395 - val_loss: 0.4459 - val_accuracy: 0.7896\n",
      "Epoch 145/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3443 - accuracy: 0.8412\n",
      "Epoch 00145: loss did not improve from 0.34418\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8408 - val_loss: 0.4534 - val_accuracy: 0.7748\n",
      "Epoch 146/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3515 - accuracy: 0.8415\n",
      "Epoch 00146: loss did not improve from 0.34418\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8408 - val_loss: 0.4451 - val_accuracy: 0.7859\n",
      "Epoch 147/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3455 - accuracy: 0.8384\n",
      "Epoch 00147: loss improved from 0.34418 to 0.34252, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3425 - accuracy: 0.8371 - val_loss: 0.4478 - val_accuracy: 0.7939\n",
      "Epoch 148/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3435 - accuracy: 0.8430\n",
      "Epoch 00148: loss did not improve from 0.34252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8403 - val_loss: 0.4523 - val_accuracy: 0.7834\n",
      "Epoch 149/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3358 - accuracy: 0.8455\n",
      "Epoch 00149: loss improved from 0.34252 to 0.33836, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.8426 - val_loss: 0.4513 - val_accuracy: 0.7834\n",
      "Epoch 150/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3335 - accuracy: 0.8464\n",
      "Epoch 00150: loss did not improve from 0.33836\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8409 - val_loss: 0.4424 - val_accuracy: 0.7933\n",
      "Epoch 151/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3360 - accuracy: 0.8398\n",
      "Epoch 00151: loss improved from 0.33836 to 0.33634, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8408 - val_loss: 0.4468 - val_accuracy: 0.7945\n",
      "Epoch 152/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3383 - accuracy: 0.8377\n",
      "Epoch 00152: loss did not improve from 0.33634\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8366 - val_loss: 0.4476 - val_accuracy: 0.7969\n",
      "Epoch 153/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3475 - accuracy: 0.8443\n",
      "Epoch 00153: loss did not improve from 0.33634\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8489 - val_loss: 0.4492 - val_accuracy: 0.7902\n",
      "Epoch 154/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3426 - accuracy: 0.8420\n",
      "Epoch 00154: loss did not improve from 0.33634\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3396 - accuracy: 0.8425 - val_loss: 0.4528 - val_accuracy: 0.7834\n",
      "Epoch 155/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3279 - accuracy: 0.8528\n",
      "Epoch 00155: loss improved from 0.33634 to 0.32969, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8512 - val_loss: 0.4449 - val_accuracy: 0.7908\n",
      "Epoch 156/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3462 - accuracy: 0.8427\n",
      "Epoch 00156: loss did not improve from 0.32969\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8469 - val_loss: 0.4512 - val_accuracy: 0.7890\n",
      "Epoch 157/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3379 - accuracy: 0.8440\n",
      "Epoch 00157: loss did not improve from 0.32969\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8392 - val_loss: 0.4575 - val_accuracy: 0.7785\n",
      "Epoch 158/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3309 - accuracy: 0.8438\n",
      "Epoch 00158: loss did not improve from 0.32969\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8441 - val_loss: 0.4541 - val_accuracy: 0.7816\n",
      "Epoch 159/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3370 - accuracy: 0.8455\n",
      "Epoch 00159: loss did not improve from 0.32969\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8451 - val_loss: 0.4515 - val_accuracy: 0.7902\n",
      "Epoch 160/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3192 - accuracy: 0.8588\n",
      "Epoch 00160: loss improved from 0.32969 to 0.32967, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3297 - accuracy: 0.8474 - val_loss: 0.4493 - val_accuracy: 0.7767\n",
      "Epoch 161/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3273 - accuracy: 0.8526\n",
      "Epoch 00161: loss improved from 0.32967 to 0.32777, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8518 - val_loss: 0.4447 - val_accuracy: 0.7926\n",
      "Epoch 162/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3377 - accuracy: 0.8405\n",
      "Epoch 00162: loss did not improve from 0.32777\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8468 - val_loss: 0.4468 - val_accuracy: 0.7963\n",
      "Epoch 163/500\n",
      "49/51 [===========================>..] - ETA: 0s - loss: 0.3273 - accuracy: 0.8511\n",
      "Epoch 00163: loss did not improve from 0.32777\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8486 - val_loss: 0.4544 - val_accuracy: 0.7840\n",
      "Epoch 164/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.3290 - accuracy: 0.8482\n",
      "Epoch 00164: loss improved from 0.32777 to 0.32761, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8477 - val_loss: 0.4517 - val_accuracy: 0.7859\n",
      "Epoch 165/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3218 - accuracy: 0.8570\n",
      "Epoch 00165: loss improved from 0.32761 to 0.32659, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8494 - val_loss: 0.4484 - val_accuracy: 0.7963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3212 - accuracy: 0.8569\n",
      "Epoch 00166: loss improved from 0.32659 to 0.32431, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3243 - accuracy: 0.8511 - val_loss: 0.4530 - val_accuracy: 0.7908\n",
      "Epoch 167/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.3164 - accuracy: 0.8585\n",
      "Epoch 00167: loss did not improve from 0.32431\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8515 - val_loss: 0.4526 - val_accuracy: 0.7847\n",
      "Epoch 168/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3246 - accuracy: 0.8448\n",
      "Epoch 00168: loss improved from 0.32431 to 0.32283, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8497 - val_loss: 0.4524 - val_accuracy: 0.7920\n",
      "Epoch 169/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3152 - accuracy: 0.8564\n",
      "Epoch 00169: loss did not improve from 0.32283\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8503 - val_loss: 0.4474 - val_accuracy: 0.7933\n",
      "Epoch 170/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3215 - accuracy: 0.8503\n",
      "Epoch 00170: loss did not improve from 0.32283\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3273 - accuracy: 0.8469 - val_loss: 0.4476 - val_accuracy: 0.7890\n",
      "Epoch 171/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3202 - accuracy: 0.8505\n",
      "Epoch 00171: loss did not improve from 0.32283\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8480 - val_loss: 0.4534 - val_accuracy: 0.7883\n",
      "Epoch 172/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3168 - accuracy: 0.8510\n",
      "Epoch 00172: loss did not improve from 0.32283\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8457 - val_loss: 0.4570 - val_accuracy: 0.7926\n",
      "Epoch 173/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3312 - accuracy: 0.8452\n",
      "Epoch 00173: loss did not improve from 0.32283\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8438 - val_loss: 0.4461 - val_accuracy: 0.7939\n",
      "Epoch 174/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3181 - accuracy: 0.8589\n",
      "Epoch 00174: loss improved from 0.32283 to 0.32111, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8561 - val_loss: 0.4411 - val_accuracy: 0.8000\n",
      "Epoch 175/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3139 - accuracy: 0.8518\n",
      "Epoch 00175: loss improved from 0.32111 to 0.31531, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3153 - accuracy: 0.8520 - val_loss: 0.4505 - val_accuracy: 0.7963\n",
      "Epoch 176/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3168 - accuracy: 0.8548\n",
      "Epoch 00176: loss did not improve from 0.31531\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.8561 - val_loss: 0.4500 - val_accuracy: 0.8006\n",
      "Epoch 177/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3219 - accuracy: 0.8466\n",
      "Epoch 00177: loss did not improve from 0.31531\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8464 - val_loss: 0.4504 - val_accuracy: 0.7982\n",
      "Epoch 178/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3157 - accuracy: 0.8584\n",
      "Epoch 00178: loss did not improve from 0.31531\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3204 - accuracy: 0.8540 - val_loss: 0.4459 - val_accuracy: 0.8043\n",
      "Epoch 179/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3134 - accuracy: 0.8531\n",
      "Epoch 00179: loss improved from 0.31531 to 0.31336, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8530 - val_loss: 0.4521 - val_accuracy: 0.7945\n",
      "Epoch 180/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3075 - accuracy: 0.8611\n",
      "Epoch 00180: loss did not improve from 0.31336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3167 - accuracy: 0.8532 - val_loss: 0.4476 - val_accuracy: 0.7988\n",
      "Epoch 181/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3187 - accuracy: 0.8499\n",
      "Epoch 00181: loss did not improve from 0.31336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3193 - accuracy: 0.8504 - val_loss: 0.4491 - val_accuracy: 0.7902\n",
      "Epoch 182/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3151 - accuracy: 0.8494\n",
      "Epoch 00182: loss did not improve from 0.31336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3139 - accuracy: 0.8538 - val_loss: 0.4480 - val_accuracy: 0.8018\n",
      "Epoch 183/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.3155 - accuracy: 0.8556\n",
      "Epoch 00183: loss did not improve from 0.31336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3151 - accuracy: 0.8557 - val_loss: 0.4457 - val_accuracy: 0.7982\n",
      "Epoch 184/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.3109 - accuracy: 0.8596\n",
      "Epoch 00184: loss did not improve from 0.31336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8543 - val_loss: 0.4554 - val_accuracy: 0.7957\n",
      "Epoch 185/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3191 - accuracy: 0.8513\n",
      "Epoch 00185: loss did not improve from 0.31336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.8507 - val_loss: 0.4471 - val_accuracy: 0.7994\n",
      "Epoch 186/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3151 - accuracy: 0.8552\n",
      "Epoch 00186: loss improved from 0.31336 to 0.31266, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8553 - val_loss: 0.4529 - val_accuracy: 0.7963\n",
      "Epoch 187/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3162 - accuracy: 0.8498\n",
      "Epoch 00187: loss did not improve from 0.31266\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3133 - accuracy: 0.8546 - val_loss: 0.4438 - val_accuracy: 0.7951\n",
      "Epoch 188/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3145 - accuracy: 0.8561\n",
      "Epoch 00188: loss did not improve from 0.31266\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8552 - val_loss: 0.4463 - val_accuracy: 0.7975\n",
      "Epoch 189/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3146 - accuracy: 0.8536\n",
      "Epoch 00189: loss did not improve from 0.31266\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8517 - val_loss: 0.4564 - val_accuracy: 0.8000\n",
      "Epoch 190/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3265 - accuracy: 0.8566\n",
      "Epoch 00190: loss did not improve from 0.31266\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3150 - accuracy: 0.8610 - val_loss: 0.4493 - val_accuracy: 0.7951\n",
      "Epoch 191/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.3012 - accuracy: 0.8608\n",
      "Epoch 00191: loss improved from 0.31266 to 0.30617, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3062 - accuracy: 0.8583 - val_loss: 0.4540 - val_accuracy: 0.7994\n",
      "Epoch 192/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.3064 - accuracy: 0.8585\n",
      "Epoch 00192: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3065 - accuracy: 0.8581 - val_loss: 0.4555 - val_accuracy: 0.8000\n",
      "Epoch 193/500\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8578\n",
      "Epoch 00193: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.8566 - val_loss: 0.4437 - val_accuracy: 0.8031\n",
      "Epoch 194/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3092 - accuracy: 0.8609\n",
      "Epoch 00194: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8593 - val_loss: 0.4467 - val_accuracy: 0.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2908 - accuracy: 0.8675\n",
      "Epoch 00195: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3078 - accuracy: 0.8599 - val_loss: 0.4509 - val_accuracy: 0.8025\n",
      "Epoch 196/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3029 - accuracy: 0.8627\n",
      "Epoch 00196: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8616 - val_loss: 0.4525 - val_accuracy: 0.8000\n",
      "Epoch 197/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3046 - accuracy: 0.8551\n",
      "Epoch 00197: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3078 - accuracy: 0.8546 - val_loss: 0.4537 - val_accuracy: 0.8049\n",
      "Epoch 198/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3099 - accuracy: 0.8627\n",
      "Epoch 00198: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.8564 - val_loss: 0.4470 - val_accuracy: 0.8012\n",
      "Epoch 199/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3100 - accuracy: 0.8576\n",
      "Epoch 00199: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8592 - val_loss: 0.4496 - val_accuracy: 0.7865\n",
      "Epoch 200/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2965 - accuracy: 0.8677\n",
      "Epoch 00200: loss did not improve from 0.30617\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8595 - val_loss: 0.4433 - val_accuracy: 0.8055\n",
      "Epoch 201/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2931 - accuracy: 0.8719\n",
      "Epoch 00201: loss improved from 0.30617 to 0.29909, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2991 - accuracy: 0.8667 - val_loss: 0.4500 - val_accuracy: 0.8012\n",
      "Epoch 202/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2937 - accuracy: 0.8692\n",
      "Epoch 00202: loss improved from 0.29909 to 0.29520, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2952 - accuracy: 0.8682 - val_loss: 0.4443 - val_accuracy: 0.8000\n",
      "Epoch 203/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2970 - accuracy: 0.8616\n",
      "Epoch 00203: loss did not improve from 0.29520\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3020 - accuracy: 0.8609 - val_loss: 0.4498 - val_accuracy: 0.8080\n",
      "Epoch 204/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2896 - accuracy: 0.8667\n",
      "Epoch 00204: loss did not improve from 0.29520\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8653 - val_loss: 0.4514 - val_accuracy: 0.7975\n",
      "Epoch 205/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2955 - accuracy: 0.8591\n",
      "Epoch 00205: loss did not improve from 0.29520\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3018 - accuracy: 0.8610 - val_loss: 0.4535 - val_accuracy: 0.8117\n",
      "Epoch 206/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.3002 - accuracy: 0.8624\n",
      "Epoch 00206: loss did not improve from 0.29520\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2955 - accuracy: 0.8638 - val_loss: 0.4545 - val_accuracy: 0.8018\n",
      "Epoch 207/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2886 - accuracy: 0.8664\n",
      "Epoch 00207: loss improved from 0.29520 to 0.29393, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2939 - accuracy: 0.8669 - val_loss: 0.4511 - val_accuracy: 0.8049\n",
      "Epoch 208/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2931 - accuracy: 0.8628\n",
      "Epoch 00208: loss did not improve from 0.29393\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.8647 - val_loss: 0.4510 - val_accuracy: 0.7951\n",
      "Epoch 209/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2967 - accuracy: 0.8662\n",
      "Epoch 00209: loss did not improve from 0.29393\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8669 - val_loss: 0.4548 - val_accuracy: 0.8067\n",
      "Epoch 210/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.3012 - accuracy: 0.8634\n",
      "Epoch 00210: loss did not improve from 0.29393\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.8609 - val_loss: 0.4523 - val_accuracy: 0.8055\n",
      "Epoch 211/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2915 - accuracy: 0.8669\n",
      "Epoch 00211: loss did not improve from 0.29393\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.8644 - val_loss: 0.4509 - val_accuracy: 0.8061\n",
      "Epoch 212/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2891 - accuracy: 0.8696\n",
      "Epoch 00212: loss did not improve from 0.29393\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.8673 - val_loss: 0.4541 - val_accuracy: 0.8031\n",
      "Epoch 213/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2810 - accuracy: 0.8705\n",
      "Epoch 00213: loss improved from 0.29393 to 0.28663, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.8690 - val_loss: 0.4517 - val_accuracy: 0.8006\n",
      "Epoch 214/500\n",
      "25/51 [=============>................] - ETA: 0s - loss: 0.2828 - accuracy: 0.8669\n",
      "Epoch 00214: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2963 - accuracy: 0.8616 - val_loss: 0.4500 - val_accuracy: 0.7963\n",
      "Epoch 215/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2929 - accuracy: 0.8649\n",
      "Epoch 00215: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2982 - accuracy: 0.8626 - val_loss: 0.4477 - val_accuracy: 0.8049\n",
      "Epoch 216/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2948 - accuracy: 0.8637\n",
      "Epoch 00216: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.8638 - val_loss: 0.4468 - val_accuracy: 0.7969\n",
      "Epoch 217/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2790 - accuracy: 0.8768\n",
      "Epoch 00217: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8669 - val_loss: 0.4548 - val_accuracy: 0.8000\n",
      "Epoch 218/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2942 - accuracy: 0.8669\n",
      "Epoch 00218: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.8682 - val_loss: 0.4580 - val_accuracy: 0.7988\n",
      "Epoch 219/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2903 - accuracy: 0.8687\n",
      "Epoch 00219: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2943 - accuracy: 0.8673 - val_loss: 0.4491 - val_accuracy: 0.8098\n",
      "Epoch 220/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2911 - accuracy: 0.8707\n",
      "Epoch 00220: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2909 - accuracy: 0.8695 - val_loss: 0.4537 - val_accuracy: 0.8074\n",
      "Epoch 221/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2894 - accuracy: 0.8687\n",
      "Epoch 00221: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.8642 - val_loss: 0.4575 - val_accuracy: 0.8135\n",
      "Epoch 222/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2955 - accuracy: 0.8652\n",
      "Epoch 00222: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2908 - accuracy: 0.8696 - val_loss: 0.4631 - val_accuracy: 0.7982\n",
      "Epoch 223/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2791 - accuracy: 0.8742\n",
      "Epoch 00223: loss did not improve from 0.28663\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8661 - val_loss: 0.4502 - val_accuracy: 0.7963\n",
      "Epoch 224/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/51 [=================>............] - ETA: 0s - loss: 0.2888 - accuracy: 0.8732\n",
      "Epoch 00224: loss improved from 0.28663 to 0.28547, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.8747 - val_loss: 0.4542 - val_accuracy: 0.8031\n",
      "Epoch 225/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2838 - accuracy: 0.8712\n",
      "Epoch 00225: loss improved from 0.28547 to 0.28522, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8707 - val_loss: 0.4589 - val_accuracy: 0.8092\n",
      "Epoch 226/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.2972 - accuracy: 0.8695\n",
      "Epoch 00226: loss improved from 0.28522 to 0.28512, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2851 - accuracy: 0.8736 - val_loss: 0.4604 - val_accuracy: 0.8061\n",
      "Epoch 227/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2870 - accuracy: 0.8721\n",
      "Epoch 00227: loss did not improve from 0.28512\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8704 - val_loss: 0.4604 - val_accuracy: 0.8043\n",
      "Epoch 228/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2659 - accuracy: 0.8826\n",
      "Epoch 00228: loss improved from 0.28512 to 0.27878, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2788 - accuracy: 0.8731 - val_loss: 0.4554 - val_accuracy: 0.8012\n",
      "Epoch 229/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2827 - accuracy: 0.8781\n",
      "Epoch 00229: loss did not improve from 0.27878\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8744 - val_loss: 0.4668 - val_accuracy: 0.8092\n",
      "Epoch 230/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2792 - accuracy: 0.8753\n",
      "Epoch 00230: loss did not improve from 0.27878\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.8699 - val_loss: 0.4600 - val_accuracy: 0.8012\n",
      "Epoch 231/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2781 - accuracy: 0.8717\n",
      "Epoch 00231: loss did not improve from 0.27878\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2847 - accuracy: 0.8681 - val_loss: 0.4582 - val_accuracy: 0.8025\n",
      "Epoch 232/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2802 - accuracy: 0.8682\n",
      "Epoch 00232: loss did not improve from 0.27878\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.8704 - val_loss: 0.4586 - val_accuracy: 0.8025\n",
      "Epoch 233/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2858 - accuracy: 0.8722\n",
      "Epoch 00233: loss did not improve from 0.27878\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.8681 - val_loss: 0.4592 - val_accuracy: 0.8000\n",
      "Epoch 234/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2724 - accuracy: 0.8800\n",
      "Epoch 00234: loss improved from 0.27878 to 0.27673, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8759 - val_loss: 0.4596 - val_accuracy: 0.8000\n",
      "Epoch 235/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2850 - accuracy: 0.8704\n",
      "Epoch 00235: loss did not improve from 0.27673\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8719 - val_loss: 0.4568 - val_accuracy: 0.8049\n",
      "Epoch 236/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2808 - accuracy: 0.8701\n",
      "Epoch 00236: loss did not improve from 0.27673\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.8762 - val_loss: 0.4580 - val_accuracy: 0.7920\n",
      "Epoch 237/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2848 - accuracy: 0.8755\n",
      "Epoch 00237: loss did not improve from 0.27673\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2881 - accuracy: 0.8722 - val_loss: 0.4601 - val_accuracy: 0.8006\n",
      "Epoch 238/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2822 - accuracy: 0.8713\n",
      "Epoch 00238: loss did not improve from 0.27673\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.8731 - val_loss: 0.4613 - val_accuracy: 0.8049\n",
      "Epoch 239/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2737 - accuracy: 0.8813\n",
      "Epoch 00239: loss did not improve from 0.27673\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2805 - accuracy: 0.8724 - val_loss: 0.4543 - val_accuracy: 0.8006\n",
      "Epoch 240/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2670 - accuracy: 0.8823\n",
      "Epoch 00240: loss improved from 0.27673 to 0.27491, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8725 - val_loss: 0.4687 - val_accuracy: 0.8000\n",
      "Epoch 241/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2806 - accuracy: 0.8712\n",
      "Epoch 00241: loss did not improve from 0.27491\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.8708 - val_loss: 0.4649 - val_accuracy: 0.7963\n",
      "Epoch 242/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2769 - accuracy: 0.8750\n",
      "Epoch 00242: loss did not improve from 0.27491\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.8751 - val_loss: 0.4547 - val_accuracy: 0.8025\n",
      "Epoch 243/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2637 - accuracy: 0.8856\n",
      "Epoch 00243: loss improved from 0.27491 to 0.27488, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2749 - accuracy: 0.8757 - val_loss: 0.4567 - val_accuracy: 0.7926\n",
      "Epoch 244/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2612 - accuracy: 0.8807\n",
      "Epoch 00244: loss improved from 0.27488 to 0.27104, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.8742 - val_loss: 0.4577 - val_accuracy: 0.7951\n",
      "Epoch 245/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2663 - accuracy: 0.8809\n",
      "Epoch 00245: loss did not improve from 0.27104\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.8774 - val_loss: 0.4567 - val_accuracy: 0.8018\n",
      "Epoch 246/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2643 - accuracy: 0.8811\n",
      "Epoch 00246: loss improved from 0.27104 to 0.26855, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2685 - accuracy: 0.8797 - val_loss: 0.4559 - val_accuracy: 0.7982\n",
      "Epoch 247/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2684 - accuracy: 0.8723\n",
      "Epoch 00247: loss did not improve from 0.26855\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2686 - accuracy: 0.8744 - val_loss: 0.4581 - val_accuracy: 0.8074\n",
      "Epoch 248/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2740 - accuracy: 0.8818\n",
      "Epoch 00248: loss did not improve from 0.26855\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.8787 - val_loss: 0.4634 - val_accuracy: 0.8086\n",
      "Epoch 249/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2698 - accuracy: 0.8773\n",
      "Epoch 00249: loss did not improve from 0.26855\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2747 - accuracy: 0.8757 - val_loss: 0.4597 - val_accuracy: 0.8147\n",
      "Epoch 250/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2791 - accuracy: 0.8705\n",
      "Epoch 00250: loss did not improve from 0.26855\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.8742 - val_loss: 0.4657 - val_accuracy: 0.8031\n",
      "Epoch 251/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2661 - accuracy: 0.8817\n",
      "Epoch 00251: loss did not improve from 0.26855\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.8742 - val_loss: 0.4639 - val_accuracy: 0.8043\n",
      "Epoch 252/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2725 - accuracy: 0.8747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00252: loss did not improve from 0.26855\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.8767 - val_loss: 0.4681 - val_accuracy: 0.8037\n",
      "Epoch 253/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2661 - accuracy: 0.8807\n",
      "Epoch 00253: loss improved from 0.26855 to 0.26755, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.8800 - val_loss: 0.4663 - val_accuracy: 0.8037\n",
      "Epoch 254/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2633 - accuracy: 0.8882\n",
      "Epoch 00254: loss did not improve from 0.26755\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2719 - accuracy: 0.8800 - val_loss: 0.4627 - val_accuracy: 0.8043\n",
      "Epoch 255/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2749 - accuracy: 0.8712\n",
      "Epoch 00255: loss did not improve from 0.26755\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.8767 - val_loss: 0.4633 - val_accuracy: 0.8037\n",
      "Epoch 256/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2752 - accuracy: 0.8767\n",
      "Epoch 00256: loss did not improve from 0.26755\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.8725 - val_loss: 0.4704 - val_accuracy: 0.8117\n",
      "Epoch 257/500\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.8747\n",
      "Epoch 00257: loss did not improve from 0.26755\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2710 - accuracy: 0.8747 - val_loss: 0.4700 - val_accuracy: 0.8049\n",
      "Epoch 258/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2565 - accuracy: 0.8809\n",
      "Epoch 00258: loss improved from 0.26755 to 0.26269, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.8785 - val_loss: 0.4634 - val_accuracy: 0.8037\n",
      "Epoch 259/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2567 - accuracy: 0.8841\n",
      "Epoch 00259: loss did not improve from 0.26269\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.8785 - val_loss: 0.4680 - val_accuracy: 0.8025\n",
      "Epoch 260/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2713 - accuracy: 0.8804\n",
      "Epoch 00260: loss did not improve from 0.26269\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8836 - val_loss: 0.4702 - val_accuracy: 0.8110\n",
      "Epoch 261/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2625 - accuracy: 0.8860\n",
      "Epoch 00261: loss did not improve from 0.26269\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.8796 - val_loss: 0.4619 - val_accuracy: 0.8123\n",
      "Epoch 262/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2633 - accuracy: 0.8853\n",
      "Epoch 00262: loss did not improve from 0.26269\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8822 - val_loss: 0.4640 - val_accuracy: 0.7988\n",
      "Epoch 263/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2609 - accuracy: 0.8840\n",
      "Epoch 00263: loss did not improve from 0.26269\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.8802 - val_loss: 0.4607 - val_accuracy: 0.8092\n",
      "Epoch 264/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2588 - accuracy: 0.8789\n",
      "Epoch 00264: loss did not improve from 0.26269\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8776 - val_loss: 0.4583 - val_accuracy: 0.8074\n",
      "Epoch 265/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2650 - accuracy: 0.8783\n",
      "Epoch 00265: loss did not improve from 0.26269\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.8787 - val_loss: 0.4679 - val_accuracy: 0.8074\n",
      "Epoch 266/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2622 - accuracy: 0.8833\n",
      "Epoch 00266: loss did not improve from 0.26269\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8822 - val_loss: 0.4608 - val_accuracy: 0.8037\n",
      "Epoch 267/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2632 - accuracy: 0.8784\n",
      "Epoch 00267: loss improved from 0.26269 to 0.25724, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2572 - accuracy: 0.8805 - val_loss: 0.4628 - val_accuracy: 0.8086\n",
      "Epoch 268/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2628 - accuracy: 0.8823\n",
      "Epoch 00268: loss did not improve from 0.25724\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8784 - val_loss: 0.4626 - val_accuracy: 0.8104\n",
      "Epoch 269/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2592 - accuracy: 0.8868\n",
      "Epoch 00269: loss did not improve from 0.25724\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8857 - val_loss: 0.4694 - val_accuracy: 0.8055\n",
      "Epoch 270/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2564 - accuracy: 0.8856\n",
      "Epoch 00270: loss did not improve from 0.25724\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2589 - accuracy: 0.8836 - val_loss: 0.4621 - val_accuracy: 0.8104\n",
      "Epoch 271/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2615 - accuracy: 0.8853\n",
      "Epoch 00271: loss did not improve from 0.25724\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2602 - accuracy: 0.8880 - val_loss: 0.4711 - val_accuracy: 0.8092\n",
      "Epoch 272/500\n",
      "26/51 [==============>...............] - ETA: 0s - loss: 0.2586 - accuracy: 0.8828\n",
      "Epoch 00272: loss did not improve from 0.25724\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2632 - accuracy: 0.8825 - val_loss: 0.4615 - val_accuracy: 0.8031\n",
      "Epoch 273/500\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.8863\n",
      "Epoch 00273: loss improved from 0.25724 to 0.25689, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.8863 - val_loss: 0.4676 - val_accuracy: 0.8061\n",
      "Epoch 274/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2631 - accuracy: 0.8785\n",
      "Epoch 00274: loss did not improve from 0.25689\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.8805 - val_loss: 0.4595 - val_accuracy: 0.8043\n",
      "Epoch 275/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2661 - accuracy: 0.8828\n",
      "Epoch 00275: loss did not improve from 0.25689\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.8803 - val_loss: 0.4583 - val_accuracy: 0.8049\n",
      "Epoch 276/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2689 - accuracy: 0.8770\n",
      "Epoch 00276: loss did not improve from 0.25689\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8810 - val_loss: 0.4733 - val_accuracy: 0.8055\n",
      "Epoch 277/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2621 - accuracy: 0.8790\n",
      "Epoch 00277: loss did not improve from 0.25689\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8793 - val_loss: 0.4743 - val_accuracy: 0.8086\n",
      "Epoch 278/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2503 - accuracy: 0.8901\n",
      "Epoch 00278: loss did not improve from 0.25689\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2600 - accuracy: 0.8820 - val_loss: 0.4587 - val_accuracy: 0.8098\n",
      "Epoch 279/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2570 - accuracy: 0.8891\n",
      "Epoch 00279: loss did not improve from 0.25689\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2602 - accuracy: 0.8827 - val_loss: 0.4666 - val_accuracy: 0.8043\n",
      "Epoch 280/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2502 - accuracy: 0.8844\n",
      "Epoch 00280: loss improved from 0.25689 to 0.25516, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2552 - accuracy: 0.8868 - val_loss: 0.4720 - val_accuracy: 0.8037\n",
      "Epoch 281/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2517 - accuracy: 0.8890\n",
      "Epoch 00281: loss did not improve from 0.25516\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.8851 - val_loss: 0.4724 - val_accuracy: 0.8074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2484 - accuracy: 0.8896\n",
      "Epoch 00282: loss did not improve from 0.25516\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2607 - accuracy: 0.8836 - val_loss: 0.4719 - val_accuracy: 0.8061\n",
      "Epoch 283/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.2473 - accuracy: 0.8903\n",
      "Epoch 00283: loss improved from 0.25516 to 0.25039, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.8879 - val_loss: 0.4696 - val_accuracy: 0.8061\n",
      "Epoch 284/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2519 - accuracy: 0.8891\n",
      "Epoch 00284: loss did not improve from 0.25039\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.8834 - val_loss: 0.4684 - val_accuracy: 0.7957\n",
      "Epoch 285/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.2537 - accuracy: 0.8883\n",
      "Epoch 00285: loss did not improve from 0.25039\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2601 - accuracy: 0.8842 - val_loss: 0.4793 - val_accuracy: 0.8031\n",
      "Epoch 286/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2526 - accuracy: 0.8877\n",
      "Epoch 00286: loss did not improve from 0.25039\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8857 - val_loss: 0.4787 - val_accuracy: 0.8104\n",
      "Epoch 287/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2433 - accuracy: 0.8921\n",
      "Epoch 00287: loss improved from 0.25039 to 0.24859, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2486 - accuracy: 0.8903 - val_loss: 0.4788 - val_accuracy: 0.8092\n",
      "Epoch 288/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2461 - accuracy: 0.8914\n",
      "Epoch 00288: loss did not improve from 0.24859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2516 - accuracy: 0.8889 - val_loss: 0.4752 - val_accuracy: 0.8110\n",
      "Epoch 289/500\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2548 - accuracy: 0.8837\n",
      "Epoch 00289: loss did not improve from 0.24859\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.8837 - val_loss: 0.4612 - val_accuracy: 0.8117\n",
      "Epoch 290/500\n",
      "25/51 [=============>................] - ETA: 0s - loss: 0.2372 - accuracy: 0.8928\n",
      "Epoch 00290: loss did not improve from 0.24859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2546 - accuracy: 0.8850 - val_loss: 0.4834 - val_accuracy: 0.8061\n",
      "Epoch 291/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2488 - accuracy: 0.8901\n",
      "Epoch 00291: loss did not improve from 0.24859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.8860 - val_loss: 0.4645 - val_accuracy: 0.8092\n",
      "Epoch 292/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2428 - accuracy: 0.8945\n",
      "Epoch 00292: loss did not improve from 0.24859\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2517 - accuracy: 0.8909 - val_loss: 0.4741 - val_accuracy: 0.8147\n",
      "Epoch 293/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2384 - accuracy: 0.8998\n",
      "Epoch 00293: loss improved from 0.24859 to 0.24252, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2425 - accuracy: 0.8923 - val_loss: 0.4690 - val_accuracy: 0.8043\n",
      "Epoch 294/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2506 - accuracy: 0.8881\n",
      "Epoch 00294: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2517 - accuracy: 0.8882 - val_loss: 0.4624 - val_accuracy: 0.8074\n",
      "Epoch 295/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2481 - accuracy: 0.8926\n",
      "Epoch 00295: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.8877 - val_loss: 0.4772 - val_accuracy: 0.8086\n",
      "Epoch 296/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2554 - accuracy: 0.8859\n",
      "Epoch 00296: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.8882 - val_loss: 0.4700 - val_accuracy: 0.7988\n",
      "Epoch 297/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2444 - accuracy: 0.8891\n",
      "Epoch 00297: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.8876 - val_loss: 0.4659 - val_accuracy: 0.8153\n",
      "Epoch 298/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2428 - accuracy: 0.8938\n",
      "Epoch 00298: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.8880 - val_loss: 0.4742 - val_accuracy: 0.8104\n",
      "Epoch 299/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2491 - accuracy: 0.8921\n",
      "Epoch 00299: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2524 - accuracy: 0.8889 - val_loss: 0.4734 - val_accuracy: 0.8104\n",
      "Epoch 300/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2411 - accuracy: 0.8909\n",
      "Epoch 00300: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2493 - accuracy: 0.8866 - val_loss: 0.4681 - val_accuracy: 0.8153\n",
      "Epoch 301/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2492 - accuracy: 0.8904\n",
      "Epoch 00301: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.8871 - val_loss: 0.4723 - val_accuracy: 0.8025\n",
      "Epoch 302/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2390 - accuracy: 0.8944\n",
      "Epoch 00302: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.8899 - val_loss: 0.4688 - val_accuracy: 0.8049\n",
      "Epoch 303/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2513 - accuracy: 0.8828\n",
      "Epoch 00303: loss did not improve from 0.24252\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2522 - accuracy: 0.8827 - val_loss: 0.4750 - val_accuracy: 0.8055\n",
      "Epoch 304/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2384 - accuracy: 0.8977\n",
      "Epoch 00304: loss improved from 0.24252 to 0.24208, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2421 - accuracy: 0.8952 - val_loss: 0.4763 - val_accuracy: 0.8141\n",
      "Epoch 305/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2487 - accuracy: 0.8924\n",
      "Epoch 00305: loss did not improve from 0.24208\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.8905 - val_loss: 0.4714 - val_accuracy: 0.8061\n",
      "Epoch 306/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2405 - accuracy: 0.8984\n",
      "Epoch 00306: loss did not improve from 0.24208\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.8931 - val_loss: 0.4660 - val_accuracy: 0.8117\n",
      "Epoch 307/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2460 - accuracy: 0.8891\n",
      "Epoch 00307: loss did not improve from 0.24208\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2491 - accuracy: 0.8903 - val_loss: 0.4640 - val_accuracy: 0.8098\n",
      "Epoch 308/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2478 - accuracy: 0.8876\n",
      "Epoch 00308: loss did not improve from 0.24208\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2510 - accuracy: 0.8897 - val_loss: 0.4644 - val_accuracy: 0.8172\n",
      "Epoch 309/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.2467 - accuracy: 0.8822\n",
      "Epoch 00309: loss did not improve from 0.24208\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2430 - accuracy: 0.8889 - val_loss: 0.4755 - val_accuracy: 0.7994\n",
      "Epoch 310/500\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.2499 - accuracy: 0.8917\n",
      "Epoch 00310: loss did not improve from 0.24208\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.8914 - val_loss: 0.4735 - val_accuracy: 0.8160\n",
      "Epoch 311/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/51 [==============>...............] - ETA: 0s - loss: 0.2597 - accuracy: 0.8808\n",
      "Epoch 00311: loss did not improve from 0.24208\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.8877 - val_loss: 0.4588 - val_accuracy: 0.8080\n",
      "Epoch 312/500\n",
      "51/51 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.8940\n",
      "Epoch 00312: loss did not improve from 0.24208\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.8940 - val_loss: 0.4567 - val_accuracy: 0.8147\n",
      "Epoch 313/500\n",
      "26/51 [==============>...............] - ETA: 0s - loss: 0.2275 - accuracy: 0.9011\n",
      "Epoch 00313: loss improved from 0.24208 to 0.24039, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2404 - accuracy: 0.8902 - val_loss: 0.4706 - val_accuracy: 0.8086\n",
      "Epoch 314/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2473 - accuracy: 0.8968\n",
      "Epoch 00314: loss did not improve from 0.24039\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.8938 - val_loss: 0.4721 - val_accuracy: 0.8061\n",
      "Epoch 315/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2415 - accuracy: 0.8948\n",
      "Epoch 00315: loss did not improve from 0.24039\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2409 - accuracy: 0.8945 - val_loss: 0.4690 - val_accuracy: 0.8104\n",
      "Epoch 316/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2410 - accuracy: 0.8974\n",
      "Epoch 00316: loss did not improve from 0.24039\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.8925 - val_loss: 0.4762 - val_accuracy: 0.8031\n",
      "Epoch 317/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2288 - accuracy: 0.9042\n",
      "Epoch 00317: loss improved from 0.24039 to 0.23822, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.8955 - val_loss: 0.4778 - val_accuracy: 0.8104\n",
      "Epoch 318/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2324 - accuracy: 0.8984\n",
      "Epoch 00318: loss improved from 0.23822 to 0.23789, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2379 - accuracy: 0.8980 - val_loss: 0.4667 - val_accuracy: 0.8067\n",
      "Epoch 319/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2328 - accuracy: 0.8943\n",
      "Epoch 00319: loss did not improve from 0.23789\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2432 - accuracy: 0.8925 - val_loss: 0.4775 - val_accuracy: 0.8202\n",
      "Epoch 320/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2350 - accuracy: 0.8982\n",
      "Epoch 00320: loss improved from 0.23789 to 0.23705, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.8971 - val_loss: 0.4654 - val_accuracy: 0.8117\n",
      "Epoch 321/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2346 - accuracy: 0.9008\n",
      "Epoch 00321: loss did not improve from 0.23705\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.8951 - val_loss: 0.4590 - val_accuracy: 0.8110\n",
      "Epoch 322/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2348 - accuracy: 0.8965\n",
      "Epoch 00322: loss improved from 0.23705 to 0.23628, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.8978 - val_loss: 0.4742 - val_accuracy: 0.8166\n",
      "Epoch 323/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2365 - accuracy: 0.8971\n",
      "Epoch 00323: loss did not improve from 0.23628\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2387 - accuracy: 0.8926 - val_loss: 0.4772 - val_accuracy: 0.8043\n",
      "Epoch 324/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2475 - accuracy: 0.8922\n",
      "Epoch 00324: loss did not improve from 0.23628\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.8923 - val_loss: 0.4742 - val_accuracy: 0.8129\n",
      "Epoch 325/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2246 - accuracy: 0.9015\n",
      "Epoch 00325: loss improved from 0.23628 to 0.23262, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.8983 - val_loss: 0.4706 - val_accuracy: 0.8098\n",
      "Epoch 326/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2321 - accuracy: 0.8961\n",
      "Epoch 00326: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2407 - accuracy: 0.8894 - val_loss: 0.4632 - val_accuracy: 0.8141\n",
      "Epoch 327/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2377 - accuracy: 0.8976\n",
      "Epoch 00327: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2396 - accuracy: 0.8952 - val_loss: 0.4717 - val_accuracy: 0.8123\n",
      "Epoch 328/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2287 - accuracy: 0.9008\n",
      "Epoch 00328: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2330 - accuracy: 0.8969 - val_loss: 0.4714 - val_accuracy: 0.8135\n",
      "Epoch 329/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2301 - accuracy: 0.9017\n",
      "Epoch 00329: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2326 - accuracy: 0.8994 - val_loss: 0.4878 - val_accuracy: 0.8123\n",
      "Epoch 330/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.2366 - accuracy: 0.8944\n",
      "Epoch 00330: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.8969 - val_loss: 0.4657 - val_accuracy: 0.8110\n",
      "Epoch 331/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2341 - accuracy: 0.8952\n",
      "Epoch 00331: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2388 - accuracy: 0.8906 - val_loss: 0.4704 - val_accuracy: 0.8129\n",
      "Epoch 332/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2329 - accuracy: 0.8984\n",
      "Epoch 00332: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.8952 - val_loss: 0.4822 - val_accuracy: 0.8086\n",
      "Epoch 333/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2334 - accuracy: 0.8962\n",
      "Epoch 00333: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2374 - accuracy: 0.8949 - val_loss: 0.4729 - val_accuracy: 0.8184\n",
      "Epoch 334/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2280 - accuracy: 0.8990\n",
      "Epoch 00334: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.8969 - val_loss: 0.4593 - val_accuracy: 0.8147\n",
      "Epoch 335/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2320 - accuracy: 0.8997\n",
      "Epoch 00335: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.8955 - val_loss: 0.4708 - val_accuracy: 0.8110\n",
      "Epoch 336/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2404 - accuracy: 0.8945\n",
      "Epoch 00336: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.8958 - val_loss: 0.4795 - val_accuracy: 0.8209\n",
      "Epoch 337/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2335 - accuracy: 0.8947\n",
      "Epoch 00337: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.8968 - val_loss: 0.4711 - val_accuracy: 0.8209\n",
      "Epoch 338/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2254 - accuracy: 0.8992\n",
      "Epoch 00338: loss did not improve from 0.23262\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2330 - accuracy: 0.8951 - val_loss: 0.4806 - val_accuracy: 0.8202\n",
      "Epoch 339/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2292 - accuracy: 0.9010\n",
      "Epoch 00339: loss improved from 0.23262 to 0.22897, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.8989 - val_loss: 0.4803 - val_accuracy: 0.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 340/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2334 - accuracy: 0.8926\n",
      "Epoch 00340: loss did not improve from 0.22897\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.8931 - val_loss: 0.4664 - val_accuracy: 0.8184\n",
      "Epoch 341/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2309 - accuracy: 0.8982\n",
      "Epoch 00341: loss improved from 0.22897 to 0.22613, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2261 - accuracy: 0.9011 - val_loss: 0.4758 - val_accuracy: 0.8147\n",
      "Epoch 342/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2282 - accuracy: 0.9050\n",
      "Epoch 00342: loss did not improve from 0.22613\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2290 - accuracy: 0.9026 - val_loss: 0.4728 - val_accuracy: 0.8166\n",
      "Epoch 343/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2344 - accuracy: 0.8990\n",
      "Epoch 00343: loss did not improve from 0.22613\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2330 - accuracy: 0.8981 - val_loss: 0.4686 - val_accuracy: 0.8080\n",
      "Epoch 344/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2216 - accuracy: 0.9075\n",
      "Epoch 00344: loss improved from 0.22613 to 0.22560, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.9024 - val_loss: 0.4796 - val_accuracy: 0.8129\n",
      "Epoch 345/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2356 - accuracy: 0.9001\n",
      "Epoch 00345: loss did not improve from 0.22560\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.9008 - val_loss: 0.4750 - val_accuracy: 0.8049\n",
      "Epoch 346/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2208 - accuracy: 0.8997\n",
      "Epoch 00346: loss improved from 0.22560 to 0.22481, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.8986 - val_loss: 0.4884 - val_accuracy: 0.8092\n",
      "Epoch 347/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2236 - accuracy: 0.9057\n",
      "Epoch 00347: loss did not improve from 0.22481\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.9004 - val_loss: 0.4762 - val_accuracy: 0.8178\n",
      "Epoch 348/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2267 - accuracy: 0.8990\n",
      "Epoch 00348: loss did not improve from 0.22481\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2262 - accuracy: 0.8997 - val_loss: 0.4842 - val_accuracy: 0.8184\n",
      "Epoch 349/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2156 - accuracy: 0.9071\n",
      "Epoch 00349: loss improved from 0.22481 to 0.22392, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9041 - val_loss: 0.4732 - val_accuracy: 0.8166\n",
      "Epoch 350/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2282 - accuracy: 0.9026\n",
      "Epoch 00350: loss improved from 0.22392 to 0.22289, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.9037 - val_loss: 0.4720 - val_accuracy: 0.8067\n",
      "Epoch 351/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2237 - accuracy: 0.9046\n",
      "Epoch 00351: loss did not improve from 0.22289\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2240 - accuracy: 0.9031 - val_loss: 0.4808 - val_accuracy: 0.8160\n",
      "Epoch 352/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2244 - accuracy: 0.9035\n",
      "Epoch 00352: loss did not improve from 0.22289\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.9017 - val_loss: 0.4662 - val_accuracy: 0.8172\n",
      "Epoch 353/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2284 - accuracy: 0.9008\n",
      "Epoch 00353: loss improved from 0.22289 to 0.22266, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2227 - accuracy: 0.9024 - val_loss: 0.4928 - val_accuracy: 0.8196\n",
      "Epoch 354/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2311 - accuracy: 0.8992\n",
      "Epoch 00354: loss did not improve from 0.22266\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.8997 - val_loss: 0.4819 - val_accuracy: 0.8147\n",
      "Epoch 355/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2362 - accuracy: 0.8948\n",
      "Epoch 00355: loss did not improve from 0.22266\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2323 - accuracy: 0.8978 - val_loss: 0.4838 - val_accuracy: 0.8153\n",
      "Epoch 356/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2165 - accuracy: 0.9078\n",
      "Epoch 00356: loss improved from 0.22266 to 0.22133, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2213 - accuracy: 0.9046 - val_loss: 0.4873 - val_accuracy: 0.8018\n",
      "Epoch 357/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2239 - accuracy: 0.9010\n",
      "Epoch 00357: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2283 - accuracy: 0.9006 - val_loss: 0.4856 - val_accuracy: 0.8178\n",
      "Epoch 358/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2276 - accuracy: 0.8964\n",
      "Epoch 00358: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2291 - accuracy: 0.8965 - val_loss: 0.4859 - val_accuracy: 0.8147\n",
      "Epoch 359/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2271 - accuracy: 0.8998\n",
      "Epoch 00359: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.8980 - val_loss: 0.4827 - val_accuracy: 0.8160\n",
      "Epoch 360/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2183 - accuracy: 0.9081\n",
      "Epoch 00360: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9027 - val_loss: 0.4878 - val_accuracy: 0.8166\n",
      "Epoch 361/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2267 - accuracy: 0.9047\n",
      "Epoch 00361: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9009 - val_loss: 0.4863 - val_accuracy: 0.8135\n",
      "Epoch 362/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2229 - accuracy: 0.9050\n",
      "Epoch 00362: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2268 - accuracy: 0.9018 - val_loss: 0.4809 - val_accuracy: 0.8049\n",
      "Epoch 363/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2201 - accuracy: 0.9060\n",
      "Epoch 00363: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2242 - accuracy: 0.9029 - val_loss: 0.4955 - val_accuracy: 0.8061\n",
      "Epoch 364/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2240 - accuracy: 0.9057\n",
      "Epoch 00364: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9021 - val_loss: 0.4771 - val_accuracy: 0.8043\n",
      "Epoch 365/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2250 - accuracy: 0.9013\n",
      "Epoch 00365: loss did not improve from 0.22133\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.8992 - val_loss: 0.4807 - val_accuracy: 0.8215\n",
      "Epoch 366/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2143 - accuracy: 0.9070\n",
      "Epoch 00366: loss improved from 0.22133 to 0.22035, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9031 - val_loss: 0.4753 - val_accuracy: 0.8135\n",
      "Epoch 367/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2219 - accuracy: 0.9044\n",
      "Epoch 00367: loss did not improve from 0.22035\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2267 - accuracy: 0.9011 - val_loss: 0.4729 - val_accuracy: 0.8110\n",
      "Epoch 368/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2185 - accuracy: 0.9060\n",
      "Epoch 00368: loss did not improve from 0.22035\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2242 - accuracy: 0.9012 - val_loss: 0.4680 - val_accuracy: 0.8196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 369/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2148 - accuracy: 0.9103\n",
      "Epoch 00369: loss improved from 0.22035 to 0.21818, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.9064 - val_loss: 0.4715 - val_accuracy: 0.8202\n",
      "Epoch 370/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2261 - accuracy: 0.8944\n",
      "Epoch 00370: loss did not improve from 0.21818\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2220 - accuracy: 0.9000 - val_loss: 0.4889 - val_accuracy: 0.8172\n",
      "Epoch 371/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2193 - accuracy: 0.9057\n",
      "Epoch 00371: loss did not improve from 0.21818\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.9008 - val_loss: 0.4888 - val_accuracy: 0.8135\n",
      "Epoch 372/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2175 - accuracy: 0.9109\n",
      "Epoch 00372: loss did not improve from 0.21818\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2255 - accuracy: 0.9046 - val_loss: 0.4827 - val_accuracy: 0.8055\n",
      "Epoch 373/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2161 - accuracy: 0.9041\n",
      "Epoch 00373: loss did not improve from 0.21818\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2202 - accuracy: 0.9054 - val_loss: 0.4759 - val_accuracy: 0.8092\n",
      "Epoch 374/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2163 - accuracy: 0.9075\n",
      "Epoch 00374: loss did not improve from 0.21818\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2245 - accuracy: 0.9055 - val_loss: 0.4837 - val_accuracy: 0.8184\n",
      "Epoch 375/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2178 - accuracy: 0.9008\n",
      "Epoch 00375: loss improved from 0.21818 to 0.21517, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9034 - val_loss: 0.4804 - val_accuracy: 0.8098\n",
      "Epoch 376/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2190 - accuracy: 0.9049\n",
      "Epoch 00376: loss did not improve from 0.21517\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9046 - val_loss: 0.4806 - val_accuracy: 0.8160\n",
      "Epoch 377/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2193 - accuracy: 0.9032\n",
      "Epoch 00377: loss did not improve from 0.21517\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2159 - accuracy: 0.9043 - val_loss: 0.4948 - val_accuracy: 0.8117\n",
      "Epoch 378/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2216 - accuracy: 0.9083\n",
      "Epoch 00378: loss did not improve from 0.21517\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9057 - val_loss: 0.4714 - val_accuracy: 0.8209\n",
      "Epoch 379/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2270 - accuracy: 0.9041\n",
      "Epoch 00379: loss did not improve from 0.21517\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2242 - accuracy: 0.9029 - val_loss: 0.4833 - val_accuracy: 0.8110\n",
      "Epoch 380/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2045 - accuracy: 0.9135\n",
      "Epoch 00380: loss improved from 0.21517 to 0.21378, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9075 - val_loss: 0.4806 - val_accuracy: 0.8190\n",
      "Epoch 381/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2085 - accuracy: 0.9080\n",
      "Epoch 00381: loss improved from 0.21378 to 0.21336, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2134 - accuracy: 0.9066 - val_loss: 0.4825 - val_accuracy: 0.8307\n",
      "Epoch 382/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2106 - accuracy: 0.9078\n",
      "Epoch 00382: loss did not improve from 0.21336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9066 - val_loss: 0.4771 - val_accuracy: 0.8184\n",
      "Epoch 383/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2227 - accuracy: 0.9030\n",
      "Epoch 00383: loss did not improve from 0.21336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9035 - val_loss: 0.4807 - val_accuracy: 0.8209\n",
      "Epoch 384/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2058 - accuracy: 0.9089\n",
      "Epoch 00384: loss improved from 0.21336 to 0.20988, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9049 - val_loss: 0.4811 - val_accuracy: 0.8135\n",
      "Epoch 385/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2027 - accuracy: 0.9149\n",
      "Epoch 00385: loss did not improve from 0.20988\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2138 - accuracy: 0.9089 - val_loss: 0.4816 - val_accuracy: 0.8190\n",
      "Epoch 386/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2051 - accuracy: 0.9146\n",
      "Epoch 00386: loss did not improve from 0.20988\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.9087 - val_loss: 0.4857 - val_accuracy: 0.8135\n",
      "Epoch 387/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2145 - accuracy: 0.9105\n",
      "Epoch 00387: loss did not improve from 0.20988\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2147 - accuracy: 0.9092 - val_loss: 0.4851 - val_accuracy: 0.8301\n",
      "Epoch 388/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2163 - accuracy: 0.9043\n",
      "Epoch 00388: loss did not improve from 0.20988\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2154 - accuracy: 0.9058 - val_loss: 0.4945 - val_accuracy: 0.8233\n",
      "Epoch 389/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2071 - accuracy: 0.9098\n",
      "Epoch 00389: loss did not improve from 0.20988\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9061 - val_loss: 0.4811 - val_accuracy: 0.8153\n",
      "Epoch 390/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2154 - accuracy: 0.9108\n",
      "Epoch 00390: loss did not improve from 0.20988\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9090 - val_loss: 0.4791 - val_accuracy: 0.8184\n",
      "Epoch 391/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2264 - accuracy: 0.8972\n",
      "Epoch 00391: loss did not improve from 0.20988\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.9035 - val_loss: 0.4828 - val_accuracy: 0.8098\n",
      "Epoch 392/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2027 - accuracy: 0.9156\n",
      "Epoch 00392: loss improved from 0.20988 to 0.20866, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9110 - val_loss: 0.4857 - val_accuracy: 0.8178\n",
      "Epoch 393/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2194 - accuracy: 0.9054\n",
      "Epoch 00393: loss did not improve from 0.20866\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2167 - accuracy: 0.9052 - val_loss: 0.4812 - val_accuracy: 0.8209\n",
      "Epoch 394/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2047 - accuracy: 0.9080\n",
      "Epoch 00394: loss did not improve from 0.20866\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2112 - accuracy: 0.9054 - val_loss: 0.4844 - val_accuracy: 0.8135\n",
      "Epoch 395/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2190 - accuracy: 0.9055\n",
      "Epoch 00395: loss did not improve from 0.20866\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.9035 - val_loss: 0.4911 - val_accuracy: 0.8172\n",
      "Epoch 396/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2059 - accuracy: 0.9138\n",
      "Epoch 00396: loss improved from 0.20866 to 0.20721, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2072 - accuracy: 0.9126 - val_loss: 0.4913 - val_accuracy: 0.8221\n",
      "Epoch 397/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.2055 - accuracy: 0.9104\n",
      "Epoch 00397: loss did not improve from 0.20721\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2122 - accuracy: 0.9077 - val_loss: 0.4845 - val_accuracy: 0.8172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1950 - accuracy: 0.9103\n",
      "Epoch 00398: loss improved from 0.20721 to 0.20600, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2060 - accuracy: 0.9070 - val_loss: 0.4908 - val_accuracy: 0.8141\n",
      "Epoch 399/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2035 - accuracy: 0.9100\n",
      "Epoch 00399: loss did not improve from 0.20600\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9047 - val_loss: 0.4835 - val_accuracy: 0.8190\n",
      "Epoch 400/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2135 - accuracy: 0.9044\n",
      "Epoch 00400: loss did not improve from 0.20600\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2108 - accuracy: 0.9072 - val_loss: 0.4870 - val_accuracy: 0.8227\n",
      "Epoch 401/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2068 - accuracy: 0.9057\n",
      "Epoch 00401: loss did not improve from 0.20600\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9072 - val_loss: 0.4837 - val_accuracy: 0.8166\n",
      "Epoch 402/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2137 - accuracy: 0.9113\n",
      "Epoch 00402: loss did not improve from 0.20600\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2120 - accuracy: 0.9116 - val_loss: 0.4931 - val_accuracy: 0.8209\n",
      "Epoch 403/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2196 - accuracy: 0.9103\n",
      "Epoch 00403: loss did not improve from 0.20600\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9090 - val_loss: 0.4937 - val_accuracy: 0.8153\n",
      "Epoch 404/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2141 - accuracy: 0.9130\n",
      "Epoch 00404: loss did not improve from 0.20600\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2099 - accuracy: 0.9121 - val_loss: 0.4987 - val_accuracy: 0.8135\n",
      "Epoch 405/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1960 - accuracy: 0.9156\n",
      "Epoch 00405: loss improved from 0.20600 to 0.20527, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9101 - val_loss: 0.5123 - val_accuracy: 0.8190\n",
      "Epoch 406/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2067 - accuracy: 0.9110\n",
      "Epoch 00406: loss did not improve from 0.20527\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2075 - accuracy: 0.9106 - val_loss: 0.5037 - val_accuracy: 0.8215\n",
      "Epoch 407/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1976 - accuracy: 0.9163\n",
      "Epoch 00407: loss did not improve from 0.20527\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9107 - val_loss: 0.5037 - val_accuracy: 0.8160\n",
      "Epoch 408/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1994 - accuracy: 0.9143\n",
      "Epoch 00408: loss improved from 0.20527 to 0.20360, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9115 - val_loss: 0.4951 - val_accuracy: 0.8178\n",
      "Epoch 409/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2002 - accuracy: 0.9125\n",
      "Epoch 00409: loss did not improve from 0.20360\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2079 - accuracy: 0.9081 - val_loss: 0.5015 - val_accuracy: 0.8172\n",
      "Epoch 410/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2095 - accuracy: 0.9122\n",
      "Epoch 00410: loss did not improve from 0.20360\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2101 - accuracy: 0.9096 - val_loss: 0.4906 - val_accuracy: 0.8196\n",
      "Epoch 411/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2113 - accuracy: 0.9086\n",
      "Epoch 00411: loss did not improve from 0.20360\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2121 - accuracy: 0.9069 - val_loss: 0.4934 - val_accuracy: 0.8209\n",
      "Epoch 412/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1996 - accuracy: 0.9156\n",
      "Epoch 00412: loss did not improve from 0.20360\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2040 - accuracy: 0.9123 - val_loss: 0.4902 - val_accuracy: 0.8233\n",
      "Epoch 413/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2169 - accuracy: 0.9060\n",
      "Epoch 00413: loss did not improve from 0.20360\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9055 - val_loss: 0.5037 - val_accuracy: 0.8221\n",
      "Epoch 414/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2039 - accuracy: 0.9128\n",
      "Epoch 00414: loss did not improve from 0.20360\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2077 - accuracy: 0.9101 - val_loss: 0.4977 - val_accuracy: 0.8252\n",
      "Epoch 415/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2010 - accuracy: 0.9156\n",
      "Epoch 00415: loss improved from 0.20360 to 0.20336, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9172 - val_loss: 0.4991 - val_accuracy: 0.8202\n",
      "Epoch 416/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2138 - accuracy: 0.9133\n",
      "Epoch 00416: loss did not improve from 0.20336\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2154 - accuracy: 0.9081 - val_loss: 0.4964 - val_accuracy: 0.8196\n",
      "Epoch 417/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2013 - accuracy: 0.9073\n",
      "Epoch 00417: loss improved from 0.20336 to 0.20259, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9104 - val_loss: 0.5000 - val_accuracy: 0.8221\n",
      "Epoch 418/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.2065 - accuracy: 0.9120\n",
      "Epoch 00418: loss did not improve from 0.20259\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2065 - accuracy: 0.9078 - val_loss: 0.5025 - val_accuracy: 0.8252\n",
      "Epoch 419/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2060 - accuracy: 0.9141\n",
      "Epoch 00419: loss did not improve from 0.20259\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2041 - accuracy: 0.9130 - val_loss: 0.5004 - val_accuracy: 0.8129\n",
      "Epoch 420/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2143 - accuracy: 0.9126\n",
      "Epoch 00420: loss did not improve from 0.20259\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2104 - accuracy: 0.9127 - val_loss: 0.4932 - val_accuracy: 0.8166\n",
      "Epoch 421/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2059 - accuracy: 0.9113\n",
      "Epoch 00421: loss did not improve from 0.20259\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2053 - accuracy: 0.9107 - val_loss: 0.4940 - val_accuracy: 0.8325\n",
      "Epoch 422/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1993 - accuracy: 0.9154\n",
      "Epoch 00422: loss improved from 0.20259 to 0.19795, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1980 - accuracy: 0.9170 - val_loss: 0.4973 - val_accuracy: 0.8215\n",
      "Epoch 423/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.2060 - accuracy: 0.9068\n",
      "Epoch 00423: loss did not improve from 0.19795\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2128 - accuracy: 0.9038 - val_loss: 0.4876 - val_accuracy: 0.8282\n",
      "Epoch 424/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2017 - accuracy: 0.9156\n",
      "Epoch 00424: loss did not improve from 0.19795\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.9123 - val_loss: 0.4875 - val_accuracy: 0.8276\n",
      "Epoch 425/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.2058 - accuracy: 0.9118\n",
      "Epoch 00425: loss did not improve from 0.19795\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9129 - val_loss: 0.4864 - val_accuracy: 0.8196\n",
      "Epoch 426/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1931 - accuracy: 0.9141\n",
      "Epoch 00426: loss did not improve from 0.19795\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1983 - accuracy: 0.9126 - val_loss: 0.4827 - val_accuracy: 0.8245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427/500\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9117\n",
      "Epoch 00427: loss did not improve from 0.19795\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9123 - val_loss: 0.4899 - val_accuracy: 0.8190\n",
      "Epoch 428/500\n",
      "25/51 [=============>................] - ETA: 0s - loss: 0.1965 - accuracy: 0.9162\n",
      "Epoch 00428: loss improved from 0.19795 to 0.19645, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9158 - val_loss: 0.5040 - val_accuracy: 0.8153\n",
      "Epoch 429/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2071 - accuracy: 0.9103\n",
      "Epoch 00429: loss did not improve from 0.19645\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2003 - accuracy: 0.9147 - val_loss: 0.4942 - val_accuracy: 0.8209\n",
      "Epoch 430/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1927 - accuracy: 0.9171\n",
      "Epoch 00430: loss did not improve from 0.19645\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1967 - accuracy: 0.9158 - val_loss: 0.5004 - val_accuracy: 0.8258\n",
      "Epoch 431/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2030 - accuracy: 0.9098\n",
      "Epoch 00431: loss did not improve from 0.19645\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9081 - val_loss: 0.4991 - val_accuracy: 0.8252\n",
      "Epoch 432/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1949 - accuracy: 0.9164\n",
      "Epoch 00432: loss did not improve from 0.19645\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2006 - accuracy: 0.9150 - val_loss: 0.5133 - val_accuracy: 0.8202\n",
      "Epoch 433/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1952 - accuracy: 0.9203\n",
      "Epoch 00433: loss did not improve from 0.19645\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.9167 - val_loss: 0.4992 - val_accuracy: 0.8301\n",
      "Epoch 434/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1893 - accuracy: 0.9162\n",
      "Epoch 00434: loss did not improve from 0.19645\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2004 - accuracy: 0.9146 - val_loss: 0.4874 - val_accuracy: 0.8258\n",
      "Epoch 435/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.1950 - accuracy: 0.9187\n",
      "Epoch 00435: loss improved from 0.19645 to 0.19522, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1952 - accuracy: 0.9173 - val_loss: 0.4943 - val_accuracy: 0.8264\n",
      "Epoch 436/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1930 - accuracy: 0.9227\n",
      "Epoch 00436: loss improved from 0.19522 to 0.19416, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1942 - accuracy: 0.9210 - val_loss: 0.5091 - val_accuracy: 0.8202\n",
      "Epoch 437/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2084 - accuracy: 0.9073\n",
      "Epoch 00437: loss did not improve from 0.19416\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.9101 - val_loss: 0.5057 - val_accuracy: 0.8209\n",
      "Epoch 438/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.2013 - accuracy: 0.9132\n",
      "Epoch 00438: loss did not improve from 0.19416\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2042 - accuracy: 0.9127 - val_loss: 0.4965 - val_accuracy: 0.8166\n",
      "Epoch 439/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1964 - accuracy: 0.9190\n",
      "Epoch 00439: loss did not improve from 0.19416\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9149 - val_loss: 0.5192 - val_accuracy: 0.8264\n",
      "Epoch 440/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.1880 - accuracy: 0.9230\n",
      "Epoch 00440: loss did not improve from 0.19416\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1984 - accuracy: 0.9153 - val_loss: 0.5011 - val_accuracy: 0.8227\n",
      "Epoch 441/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1879 - accuracy: 0.9197\n",
      "Epoch 00441: loss improved from 0.19416 to 0.19124, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1912 - accuracy: 0.9181 - val_loss: 0.4929 - val_accuracy: 0.8190\n",
      "Epoch 442/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1855 - accuracy: 0.9248\n",
      "Epoch 00442: loss did not improve from 0.19124\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1954 - accuracy: 0.9199 - val_loss: 0.5048 - val_accuracy: 0.8245\n",
      "Epoch 443/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1978 - accuracy: 0.9178\n",
      "Epoch 00443: loss did not improve from 0.19124\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9135 - val_loss: 0.5035 - val_accuracy: 0.8209\n",
      "Epoch 444/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1949 - accuracy: 0.9164\n",
      "Epoch 00444: loss did not improve from 0.19124\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.9143 - val_loss: 0.4925 - val_accuracy: 0.8245\n",
      "Epoch 445/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1988 - accuracy: 0.9181\n",
      "Epoch 00445: loss did not improve from 0.19124\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.9135 - val_loss: 0.4897 - val_accuracy: 0.8245\n",
      "Epoch 446/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1892 - accuracy: 0.9163\n",
      "Epoch 00446: loss did not improve from 0.19124\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9124 - val_loss: 0.4960 - val_accuracy: 0.8196\n",
      "Epoch 447/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1891 - accuracy: 0.9219\n",
      "Epoch 00447: loss did not improve from 0.19124\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9202 - val_loss: 0.4997 - val_accuracy: 0.8239\n",
      "Epoch 448/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1863 - accuracy: 0.9250\n",
      "Epoch 00448: loss improved from 0.19124 to 0.18914, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1891 - accuracy: 0.9216 - val_loss: 0.5038 - val_accuracy: 0.8233\n",
      "Epoch 449/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1882 - accuracy: 0.9191\n",
      "Epoch 00449: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1946 - accuracy: 0.9150 - val_loss: 0.4970 - val_accuracy: 0.8178\n",
      "Epoch 450/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1943 - accuracy: 0.9170\n",
      "Epoch 00450: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9130 - val_loss: 0.5000 - val_accuracy: 0.8264\n",
      "Epoch 451/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.2008 - accuracy: 0.9122\n",
      "Epoch 00451: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2039 - accuracy: 0.9089 - val_loss: 0.4984 - val_accuracy: 0.8264\n",
      "Epoch 452/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.1904 - accuracy: 0.9164\n",
      "Epoch 00452: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1963 - accuracy: 0.9153 - val_loss: 0.5153 - val_accuracy: 0.8331\n",
      "Epoch 453/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1950 - accuracy: 0.9119\n",
      "Epoch 00453: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1934 - accuracy: 0.9143 - val_loss: 0.5072 - val_accuracy: 0.8233\n",
      "Epoch 454/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1883 - accuracy: 0.9242\n",
      "Epoch 00454: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1981 - accuracy: 0.9176 - val_loss: 0.5054 - val_accuracy: 0.8288\n",
      "Epoch 455/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1809 - accuracy: 0.9246\n",
      "Epoch 00455: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9198 - val_loss: 0.4980 - val_accuracy: 0.8245\n",
      "Epoch 456/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/51 [===============>..............] - ETA: 0s - loss: 0.1907 - accuracy: 0.9191\n",
      "Epoch 00456: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9201 - val_loss: 0.5186 - val_accuracy: 0.8245\n",
      "Epoch 457/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.1817 - accuracy: 0.9204\n",
      "Epoch 00457: loss did not improve from 0.18914\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.9182 - val_loss: 0.5035 - val_accuracy: 0.8252\n",
      "Epoch 458/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1711 - accuracy: 0.9286\n",
      "Epoch 00458: loss improved from 0.18914 to 0.18302, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1830 - accuracy: 0.9225 - val_loss: 0.5052 - val_accuracy: 0.8276\n",
      "Epoch 459/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.1861 - accuracy: 0.9207\n",
      "Epoch 00459: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9173 - val_loss: 0.4995 - val_accuracy: 0.8301\n",
      "Epoch 460/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1868 - accuracy: 0.9216\n",
      "Epoch 00460: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1885 - accuracy: 0.9221 - val_loss: 0.5101 - val_accuracy: 0.8313\n",
      "Epoch 461/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.1862 - accuracy: 0.9224\n",
      "Epoch 00461: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1973 - accuracy: 0.9155 - val_loss: 0.4984 - val_accuracy: 0.8184\n",
      "Epoch 462/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1855 - accuracy: 0.9213\n",
      "Epoch 00462: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9204 - val_loss: 0.5098 - val_accuracy: 0.8294\n",
      "Epoch 463/500\n",
      "32/51 [=================>............] - ETA: 0s - loss: 0.1824 - accuracy: 0.9199\n",
      "Epoch 00463: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1942 - accuracy: 0.9166 - val_loss: 0.4976 - val_accuracy: 0.8276\n",
      "Epoch 464/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1844 - accuracy: 0.9181\n",
      "Epoch 00464: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1890 - accuracy: 0.9175 - val_loss: 0.5090 - val_accuracy: 0.8301\n",
      "Epoch 465/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.1787 - accuracy: 0.9219\n",
      "Epoch 00465: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1866 - accuracy: 0.9172 - val_loss: 0.4942 - val_accuracy: 0.8239\n",
      "Epoch 466/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1816 - accuracy: 0.9243\n",
      "Epoch 00466: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9187 - val_loss: 0.4895 - val_accuracy: 0.8245\n",
      "Epoch 467/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1928 - accuracy: 0.9162\n",
      "Epoch 00467: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1944 - accuracy: 0.9190 - val_loss: 0.5035 - val_accuracy: 0.8319\n",
      "Epoch 468/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.1973 - accuracy: 0.9135\n",
      "Epoch 00468: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1984 - accuracy: 0.9143 - val_loss: 0.4994 - val_accuracy: 0.8325\n",
      "Epoch 469/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1875 - accuracy: 0.9195\n",
      "Epoch 00469: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1888 - accuracy: 0.9196 - val_loss: 0.5121 - val_accuracy: 0.8209\n",
      "Epoch 470/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1925 - accuracy: 0.9172\n",
      "Epoch 00470: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1888 - accuracy: 0.9187 - val_loss: 0.5047 - val_accuracy: 0.8270\n",
      "Epoch 471/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1955 - accuracy: 0.9146\n",
      "Epoch 00471: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1974 - accuracy: 0.9121 - val_loss: 0.5045 - val_accuracy: 0.8252\n",
      "Epoch 472/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1791 - accuracy: 0.9168\n",
      "Epoch 00472: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1839 - accuracy: 0.9176 - val_loss: 0.4961 - val_accuracy: 0.8239\n",
      "Epoch 473/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1850 - accuracy: 0.9211\n",
      "Epoch 00473: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1852 - accuracy: 0.9225 - val_loss: 0.5120 - val_accuracy: 0.8276\n",
      "Epoch 474/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1824 - accuracy: 0.9199\n",
      "Epoch 00474: loss did not improve from 0.18302\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 0.9190 - val_loss: 0.5150 - val_accuracy: 0.8313\n",
      "Epoch 475/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1690 - accuracy: 0.9252\n",
      "Epoch 00475: loss improved from 0.18302 to 0.18155, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1815 - accuracy: 0.9201 - val_loss: 0.5112 - val_accuracy: 0.8356\n",
      "Epoch 476/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1735 - accuracy: 0.9289\n",
      "Epoch 00476: loss improved from 0.18155 to 0.18104, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1810 - accuracy: 0.9241 - val_loss: 0.5055 - val_accuracy: 0.8313\n",
      "Epoch 477/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1852 - accuracy: 0.9279\n",
      "Epoch 00477: loss did not improve from 0.18104\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.9216 - val_loss: 0.5079 - val_accuracy: 0.8282\n",
      "Epoch 478/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1804 - accuracy: 0.9234\n",
      "Epoch 00478: loss did not improve from 0.18104\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1826 - accuracy: 0.9233 - val_loss: 0.5235 - val_accuracy: 0.8245\n",
      "Epoch 479/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1901 - accuracy: 0.9190\n",
      "Epoch 00479: loss did not improve from 0.18104\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1932 - accuracy: 0.9161 - val_loss: 0.5012 - val_accuracy: 0.8239\n",
      "Epoch 480/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1849 - accuracy: 0.9253\n",
      "Epoch 00480: loss did not improve from 0.18104\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9222 - val_loss: 0.5082 - val_accuracy: 0.8202\n",
      "Epoch 481/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.1996 - accuracy: 0.9155\n",
      "Epoch 00481: loss did not improve from 0.18104\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.2001 - accuracy: 0.9141 - val_loss: 0.4990 - val_accuracy: 0.8245\n",
      "Epoch 482/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1972 - accuracy: 0.9201\n",
      "Epoch 00482: loss did not improve from 0.18104\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1963 - accuracy: 0.9181 - val_loss: 0.5099 - val_accuracy: 0.8227\n",
      "Epoch 483/500\n",
      "27/51 [==============>...............] - ETA: 0s - loss: 0.1923 - accuracy: 0.9135\n",
      "Epoch 00483: loss did not improve from 0.18104\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1892 - accuracy: 0.9190 - val_loss: 0.5148 - val_accuracy: 0.8313\n",
      "Epoch 484/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1793 - accuracy: 0.9259\n",
      "Epoch 00484: loss improved from 0.18104 to 0.17873, saving model to ./checkpoint\\cmLapsed.ckpt\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1787 - accuracy: 0.9258 - val_loss: 0.5018 - val_accuracy: 0.8374\n",
      "Epoch 485/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.1905 - accuracy: 0.9188\n",
      "Epoch 00485: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1916 - accuracy: 0.9184 - val_loss: 0.5037 - val_accuracy: 0.8294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1844 - accuracy: 0.9187\n",
      "Epoch 00486: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.9190 - val_loss: 0.5208 - val_accuracy: 0.8202\n",
      "Epoch 487/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1877 - accuracy: 0.9169\n",
      "Epoch 00487: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1847 - accuracy: 0.9224 - val_loss: 0.5051 - val_accuracy: 0.8294\n",
      "Epoch 488/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1770 - accuracy: 0.9211\n",
      "Epoch 00488: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9225 - val_loss: 0.5101 - val_accuracy: 0.8313\n",
      "Epoch 489/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1828 - accuracy: 0.9196\n",
      "Epoch 00489: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1853 - accuracy: 0.9207 - val_loss: 0.5151 - val_accuracy: 0.8202\n",
      "Epoch 490/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1835 - accuracy: 0.9216\n",
      "Epoch 00490: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9227 - val_loss: 0.5110 - val_accuracy: 0.8276\n",
      "Epoch 491/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1797 - accuracy: 0.9274\n",
      "Epoch 00491: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1873 - accuracy: 0.9228 - val_loss: 0.5110 - val_accuracy: 0.8362\n",
      "Epoch 492/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1821 - accuracy: 0.9224\n",
      "Epoch 00492: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9208 - val_loss: 0.5125 - val_accuracy: 0.8307\n",
      "Epoch 493/500\n",
      "29/51 [================>.............] - ETA: 0s - loss: 0.1850 - accuracy: 0.9159\n",
      "Epoch 00493: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1865 - accuracy: 0.9179 - val_loss: 0.4987 - val_accuracy: 0.8301\n",
      "Epoch 494/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1875 - accuracy: 0.9203\n",
      "Epoch 00494: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1901 - accuracy: 0.9184 - val_loss: 0.5156 - val_accuracy: 0.8325\n",
      "Epoch 495/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1783 - accuracy: 0.9231\n",
      "Epoch 00495: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1797 - accuracy: 0.9233 - val_loss: 0.5143 - val_accuracy: 0.8264\n",
      "Epoch 496/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1871 - accuracy: 0.9199\n",
      "Epoch 00496: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1861 - accuracy: 0.9208 - val_loss: 0.5086 - val_accuracy: 0.8252\n",
      "Epoch 497/500\n",
      "30/51 [================>.............] - ETA: 0s - loss: 0.1827 - accuracy: 0.9255\n",
      "Epoch 00497: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.9233 - val_loss: 0.5163 - val_accuracy: 0.8239\n",
      "Epoch 498/500\n",
      "28/51 [===============>..............] - ETA: 0s - loss: 0.1872 - accuracy: 0.9233\n",
      "Epoch 00498: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.9202 - val_loss: 0.5089 - val_accuracy: 0.8270\n",
      "Epoch 499/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1798 - accuracy: 0.9267\n",
      "Epoch 00499: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1857 - accuracy: 0.9239 - val_loss: 0.5314 - val_accuracy: 0.8178\n",
      "Epoch 500/500\n",
      "31/51 [=================>............] - ETA: 0s - loss: 0.1897 - accuracy: 0.9178\n",
      "Epoch 00500: loss did not improve from 0.17873\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.9190 - val_loss: 0.5136 - val_accuracy: 0.8270\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(2,activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_save_path + '.index'):\n",
    "    print('-------------load the model-----------------')\n",
    "    model.load_weights(checkpoint_save_path)\n",
    "    \n",
    "    \n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    ")\n",
    "   \n",
    "history = model.fit(x_train, \n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=[checkpoint], ## early_stopping,reduce_lr\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(x_valid, y_valid)\n",
    "                   ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(15, 8))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6386492252349854,\n",
       "  0.5967915058135986,\n",
       "  0.5760450959205627,\n",
       "  0.550946056842804,\n",
       "  0.5272944569587708,\n",
       "  0.5097776651382446,\n",
       "  0.5035027265548706,\n",
       "  0.4926946461200714,\n",
       "  0.4927380383014679,\n",
       "  0.4902057945728302,\n",
       "  0.4865511953830719,\n",
       "  0.4790525436401367,\n",
       "  0.4790154695510864,\n",
       "  0.4780500829219818,\n",
       "  0.47615212202072144,\n",
       "  0.4751397967338562,\n",
       "  0.47491276264190674,\n",
       "  0.4707607328891754,\n",
       "  0.4658941924571991,\n",
       "  0.4681589901447296,\n",
       "  0.46849098801612854,\n",
       "  0.4621637463569641,\n",
       "  0.4621751606464386,\n",
       "  0.4608822166919708,\n",
       "  0.4618435502052307,\n",
       "  0.4602714478969574,\n",
       "  0.45747414231300354,\n",
       "  0.45627304911613464,\n",
       "  0.45744630694389343,\n",
       "  0.448591411113739,\n",
       "  0.4555709660053253,\n",
       "  0.4489985704421997,\n",
       "  0.450141042470932,\n",
       "  0.4486364722251892,\n",
       "  0.4489694833755493,\n",
       "  0.4478601813316345,\n",
       "  0.44208791851997375,\n",
       "  0.4475579261779785,\n",
       "  0.44346123933792114,\n",
       "  0.44296637177467346,\n",
       "  0.43803921341896057,\n",
       "  0.4427184760570526,\n",
       "  0.4364130198955536,\n",
       "  0.43829599022865295,\n",
       "  0.4343266487121582,\n",
       "  0.44300374388694763,\n",
       "  0.43594789505004883,\n",
       "  0.4374539852142334,\n",
       "  0.43112650513648987,\n",
       "  0.43479660153388977,\n",
       "  0.4311981499195099,\n",
       "  0.4338708221912384,\n",
       "  0.42959049344062805,\n",
       "  0.42650169134140015,\n",
       "  0.4287632703781128,\n",
       "  0.4302142560482025,\n",
       "  0.4262503981590271,\n",
       "  0.42308178544044495,\n",
       "  0.4228622019290924,\n",
       "  0.42558079957962036,\n",
       "  0.42185500264167786,\n",
       "  0.42230507731437683,\n",
       "  0.4249982237815857,\n",
       "  0.4221312403678894,\n",
       "  0.4207649230957031,\n",
       "  0.4151516258716583,\n",
       "  0.41584688425064087,\n",
       "  0.41950660943984985,\n",
       "  0.4121711552143097,\n",
       "  0.4144538640975952,\n",
       "  0.4143252968788147,\n",
       "  0.40929532051086426,\n",
       "  0.4063594937324524,\n",
       "  0.4091351628303528,\n",
       "  0.4082663059234619,\n",
       "  0.41177836060523987,\n",
       "  0.40443840622901917,\n",
       "  0.4060452878475189,\n",
       "  0.40565943717956543,\n",
       "  0.3988892734050751,\n",
       "  0.40879586338996887,\n",
       "  0.4070906937122345,\n",
       "  0.4019917845726013,\n",
       "  0.39364176988601685,\n",
       "  0.3959442973136902,\n",
       "  0.3975116014480591,\n",
       "  0.39909252524375916,\n",
       "  0.3930402994155884,\n",
       "  0.3982570171356201,\n",
       "  0.3916465640068054,\n",
       "  0.39177730679512024,\n",
       "  0.3905772268772125,\n",
       "  0.39160048961639404,\n",
       "  0.3898278772830963,\n",
       "  0.3885636329650879,\n",
       "  0.3922816514968872,\n",
       "  0.3844556212425232,\n",
       "  0.38133519887924194,\n",
       "  0.38319113850593567,\n",
       "  0.3812457025051117,\n",
       "  0.38623717427253723,\n",
       "  0.3808267116546631,\n",
       "  0.380980521440506,\n",
       "  0.38121137022972107,\n",
       "  0.37900683283805847,\n",
       "  0.3795943558216095,\n",
       "  0.3792700469493866,\n",
       "  0.38256990909576416,\n",
       "  0.3776532709598541,\n",
       "  0.3750218451023102,\n",
       "  0.3776061534881592,\n",
       "  0.37363478541374207,\n",
       "  0.3738306164741516,\n",
       "  0.3691539466381073,\n",
       "  0.37193188071250916,\n",
       "  0.3707027733325958,\n",
       "  0.3648946285247803,\n",
       "  0.3656240403652191,\n",
       "  0.36880719661712646,\n",
       "  0.36526408791542053,\n",
       "  0.3655088245868683,\n",
       "  0.36224833130836487,\n",
       "  0.3675987720489502,\n",
       "  0.3669328987598419,\n",
       "  0.3628862202167511,\n",
       "  0.3592408001422882,\n",
       "  0.35706931352615356,\n",
       "  0.3547652065753937,\n",
       "  0.3615831732749939,\n",
       "  0.3572280704975128,\n",
       "  0.3555453419685364,\n",
       "  0.35839593410491943,\n",
       "  0.36005181074142456,\n",
       "  0.35821226239204407,\n",
       "  0.35139238834381104,\n",
       "  0.3504701852798462,\n",
       "  0.35534659028053284,\n",
       "  0.3491237163543701,\n",
       "  0.3480807840824127,\n",
       "  0.34582269191741943,\n",
       "  0.34418368339538574,\n",
       "  0.3461917042732239,\n",
       "  0.34429821372032166,\n",
       "  0.34883445501327515,\n",
       "  0.34542974829673767,\n",
       "  0.34596970677375793,\n",
       "  0.34251874685287476,\n",
       "  0.3460678160190582,\n",
       "  0.33836281299591064,\n",
       "  0.34454864263534546,\n",
       "  0.33634117245674133,\n",
       "  0.34057992696762085,\n",
       "  0.3367815613746643,\n",
       "  0.3396205008029938,\n",
       "  0.3296860158443451,\n",
       "  0.33863386511802673,\n",
       "  0.3379944860935211,\n",
       "  0.33510881662368774,\n",
       "  0.3383302688598633,\n",
       "  0.32966750860214233,\n",
       "  0.32777372002601624,\n",
       "  0.33356934785842896,\n",
       "  0.3292716145515442,\n",
       "  0.3276107907295227,\n",
       "  0.32658693194389343,\n",
       "  0.3243109881877899,\n",
       "  0.32904261350631714,\n",
       "  0.322825163602829,\n",
       "  0.32310178875923157,\n",
       "  0.3273354768753052,\n",
       "  0.32521823048591614,\n",
       "  0.32993507385253906,\n",
       "  0.3304816782474518,\n",
       "  0.32111310958862305,\n",
       "  0.3153102993965149,\n",
       "  0.3186574876308441,\n",
       "  0.32299429178237915,\n",
       "  0.32041874527931213,\n",
       "  0.3133600950241089,\n",
       "  0.31668761372566223,\n",
       "  0.31933432817459106,\n",
       "  0.3138771951198578,\n",
       "  0.315123975276947,\n",
       "  0.3190506100654602,\n",
       "  0.3173280358314514,\n",
       "  0.3126565217971802,\n",
       "  0.31334465742111206,\n",
       "  0.31651026010513306,\n",
       "  0.31771770119667053,\n",
       "  0.31495705246925354,\n",
       "  0.3061744272708893,\n",
       "  0.306516170501709,\n",
       "  0.3066824972629547,\n",
       "  0.30891701579093933,\n",
       "  0.30775830149650574,\n",
       "  0.30763307213783264,\n",
       "  0.3077651858329773,\n",
       "  0.3152052164077759,\n",
       "  0.3088723421096802,\n",
       "  0.30794066190719604,\n",
       "  0.2990928590297699,\n",
       "  0.29519572854042053,\n",
       "  0.30204999446868896,\n",
       "  0.2985750436782837,\n",
       "  0.30181998014450073,\n",
       "  0.2954752445220947,\n",
       "  0.293927401304245,\n",
       "  0.2981012761592865,\n",
       "  0.2951182723045349,\n",
       "  0.30568939447402954,\n",
       "  0.3014766275882721,\n",
       "  0.2956004738807678,\n",
       "  0.2866317927837372,\n",
       "  0.29625755548477173,\n",
       "  0.2981812059879303,\n",
       "  0.2956852912902832,\n",
       "  0.2892569899559021,\n",
       "  0.2972868084907532,\n",
       "  0.29426470398902893,\n",
       "  0.29086172580718994,\n",
       "  0.29337871074676514,\n",
       "  0.290825217962265,\n",
       "  0.29144030809402466,\n",
       "  0.28546908497810364,\n",
       "  0.28521814942359924,\n",
       "  0.2851217985153198,\n",
       "  0.2893441617488861,\n",
       "  0.2787807881832123,\n",
       "  0.2820584774017334,\n",
       "  0.2848946154117584,\n",
       "  0.2847355604171753,\n",
       "  0.2811207175254822,\n",
       "  0.28960129618644714,\n",
       "  0.2767316997051239,\n",
       "  0.2892715632915497,\n",
       "  0.27738338708877563,\n",
       "  0.28808391094207764,\n",
       "  0.2797279357910156,\n",
       "  0.28052908182144165,\n",
       "  0.2749060094356537,\n",
       "  0.28200626373291016,\n",
       "  0.27970749139785767,\n",
       "  0.2748807668685913,\n",
       "  0.2710441052913666,\n",
       "  0.2774064838886261,\n",
       "  0.2685461640357971,\n",
       "  0.26856324076652527,\n",
       "  0.27321603894233704,\n",
       "  0.27467766404151917,\n",
       "  0.2769126892089844,\n",
       "  0.2751381993293762,\n",
       "  0.27434784173965454,\n",
       "  0.26755473017692566,\n",
       "  0.27192145586013794,\n",
       "  0.27202749252319336,\n",
       "  0.2785380780696869,\n",
       "  0.2709907293319702,\n",
       "  0.2626918852329254,\n",
       "  0.26763924956321716,\n",
       "  0.2681710720062256,\n",
       "  0.2729003429412842,\n",
       "  0.2638418674468994,\n",
       "  0.26447373628616333,\n",
       "  0.2638174593448639,\n",
       "  0.26658767461776733,\n",
       "  0.2681511342525482,\n",
       "  0.25723588466644287,\n",
       "  0.2667483985424042,\n",
       "  0.26107698678970337,\n",
       "  0.25887230038642883,\n",
       "  0.2602183222770691,\n",
       "  0.2631843686103821,\n",
       "  0.25689029693603516,\n",
       "  0.2622901201248169,\n",
       "  0.26484283804893494,\n",
       "  0.261203795671463,\n",
       "  0.26441967487335205,\n",
       "  0.25998470187187195,\n",
       "  0.2601524591445923,\n",
       "  0.25515687465667725,\n",
       "  0.25761547684669495,\n",
       "  0.26073119044303894,\n",
       "  0.25039348006248474,\n",
       "  0.25350457429885864,\n",
       "  0.2601388096809387,\n",
       "  0.2578452229499817,\n",
       "  0.24859142303466797,\n",
       "  0.2516413927078247,\n",
       "  0.2547574043273926,\n",
       "  0.2546265423297882,\n",
       "  0.2551916539669037,\n",
       "  0.25165992975234985,\n",
       "  0.2425164133310318,\n",
       "  0.2517298460006714,\n",
       "  0.25067296624183655,\n",
       "  0.2528475821018219,\n",
       "  0.2496933788061142,\n",
       "  0.2481393963098526,\n",
       "  0.25235432386398315,\n",
       "  0.24931038916110992,\n",
       "  0.2518155574798584,\n",
       "  0.24375393986701965,\n",
       "  0.2521541714668274,\n",
       "  0.2420763522386551,\n",
       "  0.24738523364067078,\n",
       "  0.24639390408992767,\n",
       "  0.2490757405757904,\n",
       "  0.25102829933166504,\n",
       "  0.24301010370254517,\n",
       "  0.2502705156803131,\n",
       "  0.25286391377449036,\n",
       "  0.24269999563694,\n",
       "  0.24039484560489655,\n",
       "  0.2453511357307434,\n",
       "  0.24094663560390472,\n",
       "  0.24647490680217743,\n",
       "  0.2382190227508545,\n",
       "  0.2378903031349182,\n",
       "  0.243163600564003,\n",
       "  0.23704902827739716,\n",
       "  0.23886258900165558,\n",
       "  0.23627915978431702,\n",
       "  0.23873628675937653,\n",
       "  0.24390500783920288,\n",
       "  0.2326216995716095,\n",
       "  0.2406991571187973,\n",
       "  0.23955099284648895,\n",
       "  0.23298393189907074,\n",
       "  0.23262763023376465,\n",
       "  0.23488707840442657,\n",
       "  0.23876063525676727,\n",
       "  0.23592165112495422,\n",
       "  0.2374163717031479,\n",
       "  0.23655441403388977,\n",
       "  0.23378412425518036,\n",
       "  0.24064181745052338,\n",
       "  0.23352167010307312,\n",
       "  0.23301541805267334,\n",
       "  0.22896789014339447,\n",
       "  0.23648054897785187,\n",
       "  0.22612826526165009,\n",
       "  0.22895547747612,\n",
       "  0.2330167591571808,\n",
       "  0.2256009429693222,\n",
       "  0.23005618155002594,\n",
       "  0.2248060703277588,\n",
       "  0.23011954128742218,\n",
       "  0.22620634734630585,\n",
       "  0.22392220795154572,\n",
       "  0.22288717329502106,\n",
       "  0.22397959232330322,\n",
       "  0.2295685112476349,\n",
       "  0.22265641391277313,\n",
       "  0.2301623672246933,\n",
       "  0.23229505121707916,\n",
       "  0.22132909297943115,\n",
       "  0.22829477488994598,\n",
       "  0.2290508896112442,\n",
       "  0.22863709926605225,\n",
       "  0.22610026597976685,\n",
       "  0.22849097847938538,\n",
       "  0.22678980231285095,\n",
       "  0.22423936426639557,\n",
       "  0.22729314863681793,\n",
       "  0.23016764223575592,\n",
       "  0.22034667432308197,\n",
       "  0.22666771709918976,\n",
       "  0.22421646118164062,\n",
       "  0.21817873418331146,\n",
       "  0.2220197170972824,\n",
       "  0.2256494164466858,\n",
       "  0.22553326189517975,\n",
       "  0.220195934176445,\n",
       "  0.22454309463500977,\n",
       "  0.2151729017496109,\n",
       "  0.21864907443523407,\n",
       "  0.2158852070569992,\n",
       "  0.22092033922672272,\n",
       "  0.22415584325790405,\n",
       "  0.21377862989902496,\n",
       "  0.21336108446121216,\n",
       "  0.21516072750091553,\n",
       "  0.21857154369354248,\n",
       "  0.2098788619041443,\n",
       "  0.2137541025876999,\n",
       "  0.21233192086219788,\n",
       "  0.21472720801830292,\n",
       "  0.21541008353233337,\n",
       "  0.21442288160324097,\n",
       "  0.21490877866744995,\n",
       "  0.21929945051670074,\n",
       "  0.20865659415721893,\n",
       "  0.21674421429634094,\n",
       "  0.2112477719783783,\n",
       "  0.22046905755996704,\n",
       "  0.20721475780010223,\n",
       "  0.2121867835521698,\n",
       "  0.2059977948665619,\n",
       "  0.21023805439472198,\n",
       "  0.2108277827501297,\n",
       "  0.20940659940242767,\n",
       "  0.21200750768184662,\n",
       "  0.217141255736351,\n",
       "  0.2099287062883377,\n",
       "  0.20527182519435883,\n",
       "  0.20749595761299133,\n",
       "  0.20531874895095825,\n",
       "  0.20359914004802704,\n",
       "  0.20790326595306396,\n",
       "  0.2101166993379593,\n",
       "  0.2121337354183197,\n",
       "  0.20403486490249634,\n",
       "  0.21727389097213745,\n",
       "  0.20770207047462463,\n",
       "  0.20335766673088074,\n",
       "  0.21542038023471832,\n",
       "  0.20259325206279755,\n",
       "  0.20645803213119507,\n",
       "  0.20407532155513763,\n",
       "  0.21041947603225708,\n",
       "  0.20534610748291016,\n",
       "  0.19795171916484833,\n",
       "  0.21284092962741852,\n",
       "  0.20658625662326813,\n",
       "  0.20593716204166412,\n",
       "  0.19831594824790955,\n",
       "  0.20274996757507324,\n",
       "  0.19644884765148163,\n",
       "  0.20030364394187927,\n",
       "  0.19669854640960693,\n",
       "  0.208232119679451,\n",
       "  0.20060451328754425,\n",
       "  0.19742469489574432,\n",
       "  0.20044025778770447,\n",
       "  0.1952209770679474,\n",
       "  0.1941615343093872,\n",
       "  0.203294038772583,\n",
       "  0.20421238243579865,\n",
       "  0.20135776698589325,\n",
       "  0.19836880266666412,\n",
       "  0.19124266505241394,\n",
       "  0.19539344310760498,\n",
       "  0.20006881654262543,\n",
       "  0.19764234125614166,\n",
       "  0.20086036622524261,\n",
       "  0.19940653443336487,\n",
       "  0.19203467667102814,\n",
       "  0.18914492428302765,\n",
       "  0.19456475973129272,\n",
       "  0.19976873695850372,\n",
       "  0.20386476814746857,\n",
       "  0.19633246958255768,\n",
       "  0.1933601200580597,\n",
       "  0.19812938570976257,\n",
       "  0.19084984064102173,\n",
       "  0.19011111557483673,\n",
       "  0.19173400104045868,\n",
       "  0.18301646411418915,\n",
       "  0.19249150156974792,\n",
       "  0.18850980699062347,\n",
       "  0.19725345075130463,\n",
       "  0.19049030542373657,\n",
       "  0.19420678913593292,\n",
       "  0.1890382021665573,\n",
       "  0.18664872646331787,\n",
       "  0.19047579169273376,\n",
       "  0.19437022507190704,\n",
       "  0.19836950302124023,\n",
       "  0.18875615298748016,\n",
       "  0.18877165019512177,\n",
       "  0.19743777811527252,\n",
       "  0.18392956256866455,\n",
       "  0.18524540960788727,\n",
       "  0.1894335001707077,\n",
       "  0.18154527246952057,\n",
       "  0.18104049563407898,\n",
       "  0.1890815794467926,\n",
       "  0.18260176479816437,\n",
       "  0.19317975640296936,\n",
       "  0.1920294612646103,\n",
       "  0.2001369297504425,\n",
       "  0.1962985396385193,\n",
       "  0.1891615390777588,\n",
       "  0.1787337064743042,\n",
       "  0.191606342792511,\n",
       "  0.18571726977825165,\n",
       "  0.18470486998558044,\n",
       "  0.18510308861732483,\n",
       "  0.18534687161445618,\n",
       "  0.18540234863758087,\n",
       "  0.18734395503997803,\n",
       "  0.18318487703800201,\n",
       "  0.18650944530963898,\n",
       "  0.1900549978017807,\n",
       "  0.1797405332326889,\n",
       "  0.1860855668783188,\n",
       "  0.1860380917787552,\n",
       "  0.19193890690803528,\n",
       "  0.18565547466278076,\n",
       "  0.19001656770706177],\n",
       " 'accuracy': [0.6286240220069885,\n",
       "  0.683540403842926,\n",
       "  0.6971928477287292,\n",
       "  0.721889853477478,\n",
       "  0.7384568452835083,\n",
       "  0.7504218220710754,\n",
       "  0.754563570022583,\n",
       "  0.7590121030807495,\n",
       "  0.7617732882499695,\n",
       "  0.7631538510322571,\n",
       "  0.7633072733879089,\n",
       "  0.7688295841217041,\n",
       "  0.7726645469665527,\n",
       "  0.7649946212768555,\n",
       "  0.771897554397583,\n",
       "  0.7706703543663025,\n",
       "  0.7703635692596436,\n",
       "  0.7772664427757263,\n",
       "  0.7844761610031128,\n",
       "  0.7754256725311279,\n",
       "  0.7766528725624084,\n",
       "  0.7786470055580139,\n",
       "  0.777880072593689,\n",
       "  0.7781868577003479,\n",
       "  0.7781868577003479,\n",
       "  0.7806411981582642,\n",
       "  0.781254768371582,\n",
       "  0.7754256725311279,\n",
       "  0.7777266502380371,\n",
       "  0.7861635088920593,\n",
       "  0.7844761610031128,\n",
       "  0.7814081907272339,\n",
       "  0.7875441312789917,\n",
       "  0.7876974940299988,\n",
       "  0.7936800122261047,\n",
       "  0.7864702939987183,\n",
       "  0.7870839238166809,\n",
       "  0.7844761610031128,\n",
       "  0.790151834487915,\n",
       "  0.7890780568122864,\n",
       "  0.7880042791366577,\n",
       "  0.7852431535720825,\n",
       "  0.7944470047950745,\n",
       "  0.7889246940612793,\n",
       "  0.7880042791366577,\n",
       "  0.7906120419502258,\n",
       "  0.7927595973014832,\n",
       "  0.7904586791992188,\n",
       "  0.7939867973327637,\n",
       "  0.7933732271194458,\n",
       "  0.7965945601463318,\n",
       "  0.7858567237854004,\n",
       "  0.7953673601150513,\n",
       "  0.7984353303909302,\n",
       "  0.7970547676086426,\n",
       "  0.7947537899017334,\n",
       "  0.7992023229598999,\n",
       "  0.7969013452529907,\n",
       "  0.7993557453155518,\n",
       "  0.7976683378219604,\n",
       "  0.798895537853241,\n",
       "  0.8002761006355286,\n",
       "  0.8033440709114075,\n",
       "  0.8001227378845215,\n",
       "  0.8031906485557556,\n",
       "  0.8019635081291199,\n",
       "  0.8024236559867859,\n",
       "  0.802116870880127,\n",
       "  0.8087130188941956,\n",
       "  0.7993557453155518,\n",
       "  0.8010430932044983,\n",
       "  0.8051848411560059,\n",
       "  0.8025770783424377,\n",
       "  0.8027305006980896,\n",
       "  0.8065654039382935,\n",
       "  0.8019635081291199,\n",
       "  0.8099401593208313,\n",
       "  0.8059518337249756,\n",
       "  0.8088663816452026,\n",
       "  0.8093265891075134,\n",
       "  0.8030372858047485,\n",
       "  0.8033440709114075,\n",
       "  0.8082528114318848,\n",
       "  0.8176100850105286,\n",
       "  0.8136216998100281,\n",
       "  0.8113207817077637,\n",
       "  0.8104003667831421,\n",
       "  0.8160760998725891,\n",
       "  0.8102469444274902,\n",
       "  0.8123945593833923,\n",
       "  0.8113207817077637,\n",
       "  0.8171498775482178,\n",
       "  0.8156158924102783,\n",
       "  0.8148488998413086,\n",
       "  0.816382884979248,\n",
       "  0.8160760998725891,\n",
       "  0.8214449882507324,\n",
       "  0.819604218006134,\n",
       "  0.8235926032066345,\n",
       "  0.822365403175354,\n",
       "  0.8150023221969604,\n",
       "  0.8202178478240967,\n",
       "  0.819911003112793,\n",
       "  0.819450855255127,\n",
       "  0.8220586180686951,\n",
       "  0.8237459659576416,\n",
       "  0.8185304403305054,\n",
       "  0.8212916254997253,\n",
       "  0.8189906477928162,\n",
       "  0.8212916254997253,\n",
       "  0.8237459659576416,\n",
       "  0.8262003660202026,\n",
       "  0.8260469436645508,\n",
       "  0.8243595361709595,\n",
       "  0.8208314180374146,\n",
       "  0.8243595361709595,\n",
       "  0.8242061734199524,\n",
       "  0.8321828246116638,\n",
       "  0.8300352692604065,\n",
       "  0.8277342915534973,\n",
       "  0.8300352692604065,\n",
       "  0.8329498171806335,\n",
       "  0.8286547064781189,\n",
       "  0.8297284841537476,\n",
       "  0.8308022618293762,\n",
       "  0.8291149139404297,\n",
       "  0.8318760395050049,\n",
       "  0.8378585577011108,\n",
       "  0.831569254398346,\n",
       "  0.8329498171806335,\n",
       "  0.8332566618919373,\n",
       "  0.8327964544296265,\n",
       "  0.8332566618919373,\n",
       "  0.82834792137146,\n",
       "  0.8338702321052551,\n",
       "  0.8358644247055054,\n",
       "  0.8335634469985962,\n",
       "  0.8338702321052551,\n",
       "  0.8378585577011108,\n",
       "  0.8386255502700806,\n",
       "  0.8381653428077698,\n",
       "  0.8395459651947021,\n",
       "  0.8381653428077698,\n",
       "  0.8395459651947021,\n",
       "  0.8407731056213379,\n",
       "  0.8407731056213379,\n",
       "  0.8370915651321411,\n",
       "  0.8403129577636719,\n",
       "  0.8426138758659363,\n",
       "  0.8409265279769897,\n",
       "  0.8407731056213379,\n",
       "  0.8366313576698303,\n",
       "  0.8489031791687012,\n",
       "  0.8424605131149292,\n",
       "  0.8512041568756104,\n",
       "  0.8469090461730957,\n",
       "  0.8392391204833984,\n",
       "  0.8441478610038757,\n",
       "  0.8450682759284973,\n",
       "  0.8473692536354065,\n",
       "  0.851817786693573,\n",
       "  0.8467556238174438,\n",
       "  0.8485963940620422,\n",
       "  0.8476760387420654,\n",
       "  0.849363386631012,\n",
       "  0.8510507941246033,\n",
       "  0.8515109419822693,\n",
       "  0.8496701717376709,\n",
       "  0.8502838015556335,\n",
       "  0.8469090461730957,\n",
       "  0.8479828238487244,\n",
       "  0.8456818461418152,\n",
       "  0.8438410758972168,\n",
       "  0.8561128973960876,\n",
       "  0.8519711494445801,\n",
       "  0.8561128973960876,\n",
       "  0.8464488387107849,\n",
       "  0.8539653420448303,\n",
       "  0.8530449271202087,\n",
       "  0.8531983494758606,\n",
       "  0.8504371643066406,\n",
       "  0.8538119196891785,\n",
       "  0.8556526899337769,\n",
       "  0.8542721271514893,\n",
       "  0.8507439494132996,\n",
       "  0.8553459048271179,\n",
       "  0.8545789122581482,\n",
       "  0.8551925420761108,\n",
       "  0.8516643643379211,\n",
       "  0.8610216379165649,\n",
       "  0.858260452747345,\n",
       "  0.8581070899963379,\n",
       "  0.8565731048583984,\n",
       "  0.8593342304229736,\n",
       "  0.8599478602409363,\n",
       "  0.8616352081298828,\n",
       "  0.8545789122581482,\n",
       "  0.8564196825027466,\n",
       "  0.8591808676719666,\n",
       "  0.8594876527786255,\n",
       "  0.866697371006012,\n",
       "  0.8682312965393066,\n",
       "  0.8608682155609131,\n",
       "  0.8653167486190796,\n",
       "  0.8610216379165649,\n",
       "  0.8637827634811401,\n",
       "  0.866850733757019,\n",
       "  0.8647031784057617,\n",
       "  0.866850733757019,\n",
       "  0.8608682155609131,\n",
       "  0.8643963932991028,\n",
       "  0.8673109412193298,\n",
       "  0.8689982891082764,\n",
       "  0.8616352081298828,\n",
       "  0.8625556230545044,\n",
       "  0.8637827634811401,\n",
       "  0.866850733757019,\n",
       "  0.8682312965393066,\n",
       "  0.8673109412193298,\n",
       "  0.8694584965705872,\n",
       "  0.8642429709434509,\n",
       "  0.869611918926239,\n",
       "  0.8660837411880493,\n",
       "  0.8746740221977234,\n",
       "  0.8706856966018677,\n",
       "  0.8736002445220947,\n",
       "  0.8703789114952087,\n",
       "  0.8731400370597839,\n",
       "  0.8743672370910645,\n",
       "  0.869918704032898,\n",
       "  0.8680779337882996,\n",
       "  0.8703789114952087,\n",
       "  0.8680779337882996,\n",
       "  0.8759012222290039,\n",
       "  0.8719128966331482,\n",
       "  0.8762080073356628,\n",
       "  0.8722196817398071,\n",
       "  0.8731400370597839,\n",
       "  0.8723730444908142,\n",
       "  0.8725264668464661,\n",
       "  0.8708390593528748,\n",
       "  0.8751342296600342,\n",
       "  0.875747799873352,\n",
       "  0.8742138147354126,\n",
       "  0.8774352073669434,\n",
       "  0.8797361850738525,\n",
       "  0.8743672370910645,\n",
       "  0.8786623477935791,\n",
       "  0.875747799873352,\n",
       "  0.8742138147354126,\n",
       "  0.8742138147354126,\n",
       "  0.8766682147979736,\n",
       "  0.8800429701805115,\n",
       "  0.8800429701805115,\n",
       "  0.8766682147979736,\n",
       "  0.8725264668464661,\n",
       "  0.8746740221977234,\n",
       "  0.878508985042572,\n",
       "  0.878508985042572,\n",
       "  0.8835710883140564,\n",
       "  0.8795827627182007,\n",
       "  0.8821905255317688,\n",
       "  0.8801963329315186,\n",
       "  0.8775885701179504,\n",
       "  0.8786623477935791,\n",
       "  0.8821905255317688,\n",
       "  0.8805031180381775,\n",
       "  0.8783555626869202,\n",
       "  0.8857186436653137,\n",
       "  0.8835710883140564,\n",
       "  0.8880196213722229,\n",
       "  0.8824973106384277,\n",
       "  0.8863322734832764,\n",
       "  0.8805031180381775,\n",
       "  0.8803497552871704,\n",
       "  0.8809633255004883,\n",
       "  0.8792759776115417,\n",
       "  0.8820371031761169,\n",
       "  0.8826507329940796,\n",
       "  0.8867924809455872,\n",
       "  0.8851050734519958,\n",
       "  0.8835710883140564,\n",
       "  0.8878662586212158,\n",
       "  0.8834177255630493,\n",
       "  0.884184718132019,\n",
       "  0.8857186436653137,\n",
       "  0.8903205990791321,\n",
       "  0.8889400362968445,\n",
       "  0.8837245106697083,\n",
       "  0.884951651096344,\n",
       "  0.8860254883766174,\n",
       "  0.89093416929245,\n",
       "  0.8923147916793823,\n",
       "  0.8881730437278748,\n",
       "  0.887712836265564,\n",
       "  0.8881730437278748,\n",
       "  0.8875594139099121,\n",
       "  0.8880196213722229,\n",
       "  0.8889400362968445,\n",
       "  0.8866390585899353,\n",
       "  0.8870992660522461,\n",
       "  0.8898603916168213,\n",
       "  0.8826507329940796,\n",
       "  0.8952293395996094,\n",
       "  0.8904740214347839,\n",
       "  0.893081784248352,\n",
       "  0.8903205990791321,\n",
       "  0.8897070288658142,\n",
       "  0.8889400362968445,\n",
       "  0.8913943767547607,\n",
       "  0.887712836265564,\n",
       "  0.8940021395683289,\n",
       "  0.8901671767234802,\n",
       "  0.8938487768173218,\n",
       "  0.8944623470306396,\n",
       "  0.8924681544303894,\n",
       "  0.8955361247062683,\n",
       "  0.8979904651641846,\n",
       "  0.8924681544303894,\n",
       "  0.8970701098442078,\n",
       "  0.8950759172439575,\n",
       "  0.8978371024131775,\n",
       "  0.8926215767860413,\n",
       "  0.8923147916793823,\n",
       "  0.8982973098754883,\n",
       "  0.8894002437591553,\n",
       "  0.8952293395996094,\n",
       "  0.8969166874885559,\n",
       "  0.8993710875511169,\n",
       "  0.8969166874885559,\n",
       "  0.890627384185791,\n",
       "  0.8952293395996094,\n",
       "  0.8949225544929504,\n",
       "  0.8969166874885559,\n",
       "  0.8955361247062683,\n",
       "  0.8958429098129272,\n",
       "  0.8967633247375488,\n",
       "  0.8950759172439575,\n",
       "  0.8989108800888062,\n",
       "  0.893081784248352,\n",
       "  0.9010584354400635,\n",
       "  0.9025924205780029,\n",
       "  0.8981438875198364,\n",
       "  0.9024389982223511,\n",
       "  0.9007516503334045,\n",
       "  0.8986040949821472,\n",
       "  0.9004448652267456,\n",
       "  0.8996778726577759,\n",
       "  0.9041264057159424,\n",
       "  0.9036661982536316,\n",
       "  0.9030526280403137,\n",
       "  0.9016720652580261,\n",
       "  0.9024389982223511,\n",
       "  0.8996778726577759,\n",
       "  0.8978371024131775,\n",
       "  0.9045866131782532,\n",
       "  0.9005982279777527,\n",
       "  0.8964565396308899,\n",
       "  0.8979904651641846,\n",
       "  0.9027458429336548,\n",
       "  0.9009050726890564,\n",
       "  0.9018254280090332,\n",
       "  0.9028992056846619,\n",
       "  0.9021322131156921,\n",
       "  0.8992176651954651,\n",
       "  0.9030526280403137,\n",
       "  0.9010584354400635,\n",
       "  0.9012118577957153,\n",
       "  0.9064273834228516,\n",
       "  0.8999846577644348,\n",
       "  0.9007516503334045,\n",
       "  0.9045866131782532,\n",
       "  0.9053536057472229,\n",
       "  0.90550696849823,\n",
       "  0.9033594131469727,\n",
       "  0.9045866131782532,\n",
       "  0.9042797684669495,\n",
       "  0.9056603908538818,\n",
       "  0.9028992056846619,\n",
       "  0.9075011610984802,\n",
       "  0.9065807461738586,\n",
       "  0.9065807461738586,\n",
       "  0.9035128355026245,\n",
       "  0.9048933982849121,\n",
       "  0.9088817238807678,\n",
       "  0.9087283611297607,\n",
       "  0.9091885089874268,\n",
       "  0.9058137536048889,\n",
       "  0.9061205983161926,\n",
       "  0.9090351462364197,\n",
       "  0.9035128355026245,\n",
       "  0.9110292792320251,\n",
       "  0.905200183391571,\n",
       "  0.9053536057472229,\n",
       "  0.9035128355026245,\n",
       "  0.9125632643699646,\n",
       "  0.9076545238494873,\n",
       "  0.9070409536361694,\n",
       "  0.9047399759292603,\n",
       "  0.9071943759918213,\n",
       "  0.9071943759918213,\n",
       "  0.9116429090499878,\n",
       "  0.9090351462364197,\n",
       "  0.9121030569076538,\n",
       "  0.9101089239120483,\n",
       "  0.9105691313743591,\n",
       "  0.9107224941253662,\n",
       "  0.9114894866943359,\n",
       "  0.9081147313117981,\n",
       "  0.9096487164497375,\n",
       "  0.9068875312805176,\n",
       "  0.9122564792633057,\n",
       "  0.90550696849823,\n",
       "  0.9101089239120483,\n",
       "  0.917165219783783,\n",
       "  0.9081147313117981,\n",
       "  0.9104157090187073,\n",
       "  0.9078079462051392,\n",
       "  0.9130234718322754,\n",
       "  0.9127166867256165,\n",
       "  0.9107224941253662,\n",
       "  0.9170117974281311,\n",
       "  0.9038196206092834,\n",
       "  0.9122564792633057,\n",
       "  0.9128700494766235,\n",
       "  0.9125632643699646,\n",
       "  0.9122564792633057,\n",
       "  0.9157846570014954,\n",
       "  0.9147108197212219,\n",
       "  0.9157846570014954,\n",
       "  0.9081147313117981,\n",
       "  0.9150176644325256,\n",
       "  0.9167050123214722,\n",
       "  0.9145574569702148,\n",
       "  0.91731858253479,\n",
       "  0.9210001826286316,\n",
       "  0.9101089239120483,\n",
       "  0.9127166867256165,\n",
       "  0.9148642420768738,\n",
       "  0.9153244495391846,\n",
       "  0.9180855751037598,\n",
       "  0.9199263453483582,\n",
       "  0.9134836792945862,\n",
       "  0.9142506718635559,\n",
       "  0.9134836792945862,\n",
       "  0.9124099016189575,\n",
       "  0.9202331900596619,\n",
       "  0.9216137528419495,\n",
       "  0.9150176644325256,\n",
       "  0.9130234718322754,\n",
       "  0.9088817238807678,\n",
       "  0.9153244495391846,\n",
       "  0.9142506718635559,\n",
       "  0.9176254272460938,\n",
       "  0.9197729825973511,\n",
       "  0.92007976770401,\n",
       "  0.9182389974594116,\n",
       "  0.9225341081619263,\n",
       "  0.91731858253479,\n",
       "  0.9220739603042603,\n",
       "  0.9154778122901917,\n",
       "  0.920386552810669,\n",
       "  0.9165515899658203,\n",
       "  0.9174720048904419,\n",
       "  0.917165219783783,\n",
       "  0.9186992049217224,\n",
       "  0.9190059900283813,\n",
       "  0.9142506718635559,\n",
       "  0.9196195602416992,\n",
       "  0.9186992049217224,\n",
       "  0.9121030569076538,\n",
       "  0.9176254272460938,\n",
       "  0.9225341081619263,\n",
       "  0.9190059900283813,\n",
       "  0.92007976770401,\n",
       "  0.9240680932998657,\n",
       "  0.9216137528419495,\n",
       "  0.923301100730896,\n",
       "  0.9160914421081543,\n",
       "  0.9222273230552673,\n",
       "  0.914097249507904,\n",
       "  0.9180855751037598,\n",
       "  0.9190059900283813,\n",
       "  0.925755500793457,\n",
       "  0.9183924198150635,\n",
       "  0.9190059900283813,\n",
       "  0.9223807454109192,\n",
       "  0.9225341081619263,\n",
       "  0.9206933379173279,\n",
       "  0.9226875305175781,\n",
       "  0.92284095287323,\n",
       "  0.9208467602729797,\n",
       "  0.9179322123527527,\n",
       "  0.9183924198150635,\n",
       "  0.923301100730896,\n",
       "  0.9208467602729797,\n",
       "  0.923301100730896,\n",
       "  0.9202331900596619,\n",
       "  0.9239147305488586,\n",
       "  0.9190059900283813],\n",
       " 'val_loss': [0.616664707660675,\n",
       "  0.5846061706542969,\n",
       "  0.5658486485481262,\n",
       "  0.5339547395706177,\n",
       "  0.517340362071991,\n",
       "  0.505405068397522,\n",
       "  0.4969022572040558,\n",
       "  0.49669620394706726,\n",
       "  0.49166491627693176,\n",
       "  0.4919297993183136,\n",
       "  0.49872225522994995,\n",
       "  0.4872308373451233,\n",
       "  0.48837482929229736,\n",
       "  0.48752090334892273,\n",
       "  0.4831594228744507,\n",
       "  0.4867621064186096,\n",
       "  0.4840414822101593,\n",
       "  0.4795414209365845,\n",
       "  0.4811207056045532,\n",
       "  0.48531633615493774,\n",
       "  0.4755293130874634,\n",
       "  0.4835396707057953,\n",
       "  0.48089224100112915,\n",
       "  0.47596049308776855,\n",
       "  0.4853631258010864,\n",
       "  0.47505760192871094,\n",
       "  0.4763695001602173,\n",
       "  0.47533145546913147,\n",
       "  0.4738307595252991,\n",
       "  0.4844904839992523,\n",
       "  0.47432729601860046,\n",
       "  0.47242939472198486,\n",
       "  0.47126278281211853,\n",
       "  0.47113728523254395,\n",
       "  0.4729037582874298,\n",
       "  0.47576963901519775,\n",
       "  0.4726146459579468,\n",
       "  0.4710855185985565,\n",
       "  0.47100189328193665,\n",
       "  0.4698376953601837,\n",
       "  0.4685239791870117,\n",
       "  0.47067075967788696,\n",
       "  0.4677008390426636,\n",
       "  0.4676510691642761,\n",
       "  0.46719101071357727,\n",
       "  0.46802404522895813,\n",
       "  0.4697725772857666,\n",
       "  0.4647502899169922,\n",
       "  0.4690329432487488,\n",
       "  0.46619942784309387,\n",
       "  0.4661194980144501,\n",
       "  0.4679460823535919,\n",
       "  0.4638732075691223,\n",
       "  0.46351438760757446,\n",
       "  0.4691016972064972,\n",
       "  0.4614275395870209,\n",
       "  0.469181090593338,\n",
       "  0.46320822834968567,\n",
       "  0.46624213457107544,\n",
       "  0.46699652075767517,\n",
       "  0.4652373790740967,\n",
       "  0.4658489525318146,\n",
       "  0.4673157036304474,\n",
       "  0.4657132923603058,\n",
       "  0.4644201695919037,\n",
       "  0.4642657935619354,\n",
       "  0.4623996913433075,\n",
       "  0.4597087800502777,\n",
       "  0.4628294110298157,\n",
       "  0.461269348859787,\n",
       "  0.4620203673839569,\n",
       "  0.4579707384109497,\n",
       "  0.45618507266044617,\n",
       "  0.4584847390651703,\n",
       "  0.46178168058395386,\n",
       "  0.46213850378990173,\n",
       "  0.46380600333213806,\n",
       "  0.46140268445014954,\n",
       "  0.45714232325553894,\n",
       "  0.4624735713005066,\n",
       "  0.45516151189804077,\n",
       "  0.45477503538131714,\n",
       "  0.45736587047576904,\n",
       "  0.4599306583404541,\n",
       "  0.46112722158432007,\n",
       "  0.45968589186668396,\n",
       "  0.4546339809894562,\n",
       "  0.4557138681411743,\n",
       "  0.451110303401947,\n",
       "  0.45323726534843445,\n",
       "  0.45295315980911255,\n",
       "  0.45250871777534485,\n",
       "  0.4567238390445709,\n",
       "  0.4548805356025696,\n",
       "  0.45547038316726685,\n",
       "  0.4582067131996155,\n",
       "  0.45733699202537537,\n",
       "  0.45819687843322754,\n",
       "  0.4522484242916107,\n",
       "  0.4631066918373108,\n",
       "  0.45581531524658203,\n",
       "  0.4511324167251587,\n",
       "  0.456125408411026,\n",
       "  0.45225268602371216,\n",
       "  0.45349112153053284,\n",
       "  0.4534970223903656,\n",
       "  0.4538695812225342,\n",
       "  0.4521675705909729,\n",
       "  0.45262280106544495,\n",
       "  0.4498514235019684,\n",
       "  0.4475024938583374,\n",
       "  0.4532920718193054,\n",
       "  0.4541742503643036,\n",
       "  0.45157352089881897,\n",
       "  0.45486724376678467,\n",
       "  0.4485137462615967,\n",
       "  0.44862625002861023,\n",
       "  0.4538313150405884,\n",
       "  0.4480505585670471,\n",
       "  0.4524925947189331,\n",
       "  0.45281633734703064,\n",
       "  0.4531092345714569,\n",
       "  0.45767736434936523,\n",
       "  0.45216861367225647,\n",
       "  0.454086035490036,\n",
       "  0.4517892599105835,\n",
       "  0.45408713817596436,\n",
       "  0.4519111216068268,\n",
       "  0.4482800364494324,\n",
       "  0.447556734085083,\n",
       "  0.44571369886398315,\n",
       "  0.44661015272140503,\n",
       "  0.44470828771591187,\n",
       "  0.4474432170391083,\n",
       "  0.44732174277305603,\n",
       "  0.45465704798698425,\n",
       "  0.4551147520542145,\n",
       "  0.4513685405254364,\n",
       "  0.44725266098976135,\n",
       "  0.4498824179172516,\n",
       "  0.448317289352417,\n",
       "  0.448495477437973,\n",
       "  0.4521084725856781,\n",
       "  0.445911705493927,\n",
       "  0.45335832238197327,\n",
       "  0.445095419883728,\n",
       "  0.447769433259964,\n",
       "  0.452289342880249,\n",
       "  0.45132288336753845,\n",
       "  0.442354291677475,\n",
       "  0.4467836618423462,\n",
       "  0.4475678503513336,\n",
       "  0.44915539026260376,\n",
       "  0.45283907651901245,\n",
       "  0.44489967823028564,\n",
       "  0.45120108127593994,\n",
       "  0.45748552680015564,\n",
       "  0.4541117548942566,\n",
       "  0.45151862502098083,\n",
       "  0.4493122100830078,\n",
       "  0.4447089731693268,\n",
       "  0.44679751992225647,\n",
       "  0.45436206459999084,\n",
       "  0.45174866914749146,\n",
       "  0.44835272431373596,\n",
       "  0.4530342221260071,\n",
       "  0.45258602499961853,\n",
       "  0.4523907005786896,\n",
       "  0.44735780358314514,\n",
       "  0.4476259648799896,\n",
       "  0.4534492492675781,\n",
       "  0.45703548192977905,\n",
       "  0.4460708200931549,\n",
       "  0.4410964250564575,\n",
       "  0.45050153136253357,\n",
       "  0.44995614886283875,\n",
       "  0.45037955045700073,\n",
       "  0.44591471552848816,\n",
       "  0.4520672559738159,\n",
       "  0.4476078450679779,\n",
       "  0.4491485059261322,\n",
       "  0.44798433780670166,\n",
       "  0.44569385051727295,\n",
       "  0.455409973859787,\n",
       "  0.44713857769966125,\n",
       "  0.4529089033603668,\n",
       "  0.44381681084632874,\n",
       "  0.4462718665599823,\n",
       "  0.4563778042793274,\n",
       "  0.44927501678466797,\n",
       "  0.45400768518447876,\n",
       "  0.4555126428604126,\n",
       "  0.44373542070388794,\n",
       "  0.4467248320579529,\n",
       "  0.45094743371009827,\n",
       "  0.45248720049858093,\n",
       "  0.4537320137023926,\n",
       "  0.44698235392570496,\n",
       "  0.4496217370033264,\n",
       "  0.4433046281337738,\n",
       "  0.4499989151954651,\n",
       "  0.4442664384841919,\n",
       "  0.44984081387519836,\n",
       "  0.45135658979415894,\n",
       "  0.4534537196159363,\n",
       "  0.4545261859893799,\n",
       "  0.45114147663116455,\n",
       "  0.45095357298851013,\n",
       "  0.45481956005096436,\n",
       "  0.45234641432762146,\n",
       "  0.45092377066612244,\n",
       "  0.45413780212402344,\n",
       "  0.45171838998794556,\n",
       "  0.449975848197937,\n",
       "  0.4477381408214569,\n",
       "  0.4468057155609131,\n",
       "  0.45480626821517944,\n",
       "  0.45798036456108093,\n",
       "  0.44910958409309387,\n",
       "  0.453680157661438,\n",
       "  0.45750364661216736,\n",
       "  0.4631112217903137,\n",
       "  0.4501582682132721,\n",
       "  0.45416951179504395,\n",
       "  0.45889583230018616,\n",
       "  0.460405558347702,\n",
       "  0.460406631231308,\n",
       "  0.4554072916507721,\n",
       "  0.46678122878074646,\n",
       "  0.45995891094207764,\n",
       "  0.45822715759277344,\n",
       "  0.45861831307411194,\n",
       "  0.4591895639896393,\n",
       "  0.4595625698566437,\n",
       "  0.45681315660476685,\n",
       "  0.4580206274986267,\n",
       "  0.4600543975830078,\n",
       "  0.46126195788383484,\n",
       "  0.4542572498321533,\n",
       "  0.46866166591644287,\n",
       "  0.4648761749267578,\n",
       "  0.4546999931335449,\n",
       "  0.4566880166530609,\n",
       "  0.4576759338378906,\n",
       "  0.45669278502464294,\n",
       "  0.455892950296402,\n",
       "  0.45814988017082214,\n",
       "  0.46343764662742615,\n",
       "  0.45969849824905396,\n",
       "  0.46568185091018677,\n",
       "  0.4638613760471344,\n",
       "  0.468111127614975,\n",
       "  0.46627646684646606,\n",
       "  0.4626810550689697,\n",
       "  0.4632866084575653,\n",
       "  0.4703538715839386,\n",
       "  0.46998605132102966,\n",
       "  0.4634212255477905,\n",
       "  0.46801435947418213,\n",
       "  0.4702308773994446,\n",
       "  0.4619194269180298,\n",
       "  0.46400508284568787,\n",
       "  0.46070972084999084,\n",
       "  0.4582590162754059,\n",
       "  0.467914879322052,\n",
       "  0.460755854845047,\n",
       "  0.4628056585788727,\n",
       "  0.46264731884002686,\n",
       "  0.4694063365459442,\n",
       "  0.4621265232563019,\n",
       "  0.47109416127204895,\n",
       "  0.4615460932254791,\n",
       "  0.4676496684551239,\n",
       "  0.4594995081424713,\n",
       "  0.45825278759002686,\n",
       "  0.4733177125453949,\n",
       "  0.47429290413856506,\n",
       "  0.4586847424507141,\n",
       "  0.46659454703330994,\n",
       "  0.4720282256603241,\n",
       "  0.47240149974823,\n",
       "  0.4719373881816864,\n",
       "  0.4695630371570587,\n",
       "  0.4683724343776703,\n",
       "  0.47927504777908325,\n",
       "  0.47871044278144836,\n",
       "  0.47876664996147156,\n",
       "  0.4752485156059265,\n",
       "  0.4611889123916626,\n",
       "  0.4834207594394684,\n",
       "  0.46445432305336,\n",
       "  0.4740753471851349,\n",
       "  0.4689640998840332,\n",
       "  0.4623863995075226,\n",
       "  0.4772166907787323,\n",
       "  0.4699876308441162,\n",
       "  0.4658612906932831,\n",
       "  0.4742436110973358,\n",
       "  0.47338956594467163,\n",
       "  0.46812573075294495,\n",
       "  0.4723012447357178,\n",
       "  0.4688341021537781,\n",
       "  0.4749678075313568,\n",
       "  0.47628143429756165,\n",
       "  0.4714492857456207,\n",
       "  0.4660232663154602,\n",
       "  0.463976114988327,\n",
       "  0.46442803740501404,\n",
       "  0.47547629475593567,\n",
       "  0.47348734736442566,\n",
       "  0.4587562680244446,\n",
       "  0.4566635489463806,\n",
       "  0.47057726979255676,\n",
       "  0.47212445735931396,\n",
       "  0.4689975082874298,\n",
       "  0.47616294026374817,\n",
       "  0.4778051972389221,\n",
       "  0.4666944742202759,\n",
       "  0.47749239206314087,\n",
       "  0.4653834402561188,\n",
       "  0.4590292274951935,\n",
       "  0.4741678535938263,\n",
       "  0.4771762192249298,\n",
       "  0.4741637408733368,\n",
       "  0.47061944007873535,\n",
       "  0.4632260799407959,\n",
       "  0.471728652715683,\n",
       "  0.47137218713760376,\n",
       "  0.48782894015312195,\n",
       "  0.46569007635116577,\n",
       "  0.47036099433898926,\n",
       "  0.48222923278808594,\n",
       "  0.4728817343711853,\n",
       "  0.4592779576778412,\n",
       "  0.4708472192287445,\n",
       "  0.47946855425834656,\n",
       "  0.47107693552970886,\n",
       "  0.4805726110935211,\n",
       "  0.48026153445243835,\n",
       "  0.46643301844596863,\n",
       "  0.4757962226867676,\n",
       "  0.47277921438217163,\n",
       "  0.468605637550354,\n",
       "  0.4796498715877533,\n",
       "  0.475030779838562,\n",
       "  0.48839834332466125,\n",
       "  0.4761907458305359,\n",
       "  0.48421624302864075,\n",
       "  0.4731956124305725,\n",
       "  0.47201651334762573,\n",
       "  0.4807717204093933,\n",
       "  0.46624666452407837,\n",
       "  0.4928043782711029,\n",
       "  0.48188188672065735,\n",
       "  0.48382845520973206,\n",
       "  0.487261563539505,\n",
       "  0.48563292622566223,\n",
       "  0.48591330647468567,\n",
       "  0.4827156364917755,\n",
       "  0.4878174066543579,\n",
       "  0.48634111881256104,\n",
       "  0.48089325428009033,\n",
       "  0.4954565167427063,\n",
       "  0.47710466384887695,\n",
       "  0.48071548342704773,\n",
       "  0.47529345750808716,\n",
       "  0.4729141294956207,\n",
       "  0.468014121055603,\n",
       "  0.4715242087841034,\n",
       "  0.48888760805130005,\n",
       "  0.48879989981651306,\n",
       "  0.4827186167240143,\n",
       "  0.47593340277671814,\n",
       "  0.4837193489074707,\n",
       "  0.4803576171398163,\n",
       "  0.48059582710266113,\n",
       "  0.49484628438949585,\n",
       "  0.47144079208374023,\n",
       "  0.48332977294921875,\n",
       "  0.4806213080883026,\n",
       "  0.48248717188835144,\n",
       "  0.4771360158920288,\n",
       "  0.4807348847389221,\n",
       "  0.4810830354690552,\n",
       "  0.48155274987220764,\n",
       "  0.48569804430007935,\n",
       "  0.4851410984992981,\n",
       "  0.4945220351219177,\n",
       "  0.4811137020587921,\n",
       "  0.47913986444473267,\n",
       "  0.48280036449432373,\n",
       "  0.485718309879303,\n",
       "  0.48117193579673767,\n",
       "  0.4843515157699585,\n",
       "  0.4910571873188019,\n",
       "  0.49134233593940735,\n",
       "  0.4845370948314667,\n",
       "  0.49075913429260254,\n",
       "  0.48346608877182007,\n",
       "  0.48703470826148987,\n",
       "  0.48372527956962585,\n",
       "  0.49313440918922424,\n",
       "  0.4937456250190735,\n",
       "  0.4987296462059021,\n",
       "  0.5123258233070374,\n",
       "  0.5037209987640381,\n",
       "  0.5036945343017578,\n",
       "  0.49513065814971924,\n",
       "  0.501527726650238,\n",
       "  0.4906456768512726,\n",
       "  0.49340343475341797,\n",
       "  0.49017536640167236,\n",
       "  0.5036534070968628,\n",
       "  0.49766066670417786,\n",
       "  0.4991479814052582,\n",
       "  0.49635639786720276,\n",
       "  0.5000202655792236,\n",
       "  0.5025405287742615,\n",
       "  0.5004169940948486,\n",
       "  0.49323442578315735,\n",
       "  0.49404045939445496,\n",
       "  0.49725621938705444,\n",
       "  0.4876076281070709,\n",
       "  0.4874870479106903,\n",
       "  0.4863600730895996,\n",
       "  0.4826928675174713,\n",
       "  0.4898873269557953,\n",
       "  0.504020631313324,\n",
       "  0.4941788911819458,\n",
       "  0.5004355907440186,\n",
       "  0.4991336464881897,\n",
       "  0.5133363604545593,\n",
       "  0.4992237389087677,\n",
       "  0.48735669255256653,\n",
       "  0.49431726336479187,\n",
       "  0.5091486573219299,\n",
       "  0.5056531429290771,\n",
       "  0.49654754996299744,\n",
       "  0.5192204713821411,\n",
       "  0.5010826587677002,\n",
       "  0.4929077923297882,\n",
       "  0.5048065185546875,\n",
       "  0.5035465359687805,\n",
       "  0.49248453974723816,\n",
       "  0.4896906316280365,\n",
       "  0.4960418939590454,\n",
       "  0.4997325539588928,\n",
       "  0.5038377642631531,\n",
       "  0.49704882502555847,\n",
       "  0.5000296235084534,\n",
       "  0.49835821986198425,\n",
       "  0.5152755379676819,\n",
       "  0.5071699619293213,\n",
       "  0.5053855776786804,\n",
       "  0.49801257252693176,\n",
       "  0.5185797810554504,\n",
       "  0.5034675002098083,\n",
       "  0.505237877368927,\n",
       "  0.499463826417923,\n",
       "  0.5101057887077332,\n",
       "  0.4984354078769684,\n",
       "  0.5097578167915344,\n",
       "  0.4975714683532715,\n",
       "  0.5089921355247498,\n",
       "  0.4941706955432892,\n",
       "  0.4895038306713104,\n",
       "  0.5035185217857361,\n",
       "  0.4994373619556427,\n",
       "  0.5121040940284729,\n",
       "  0.5046671032905579,\n",
       "  0.5044940114021301,\n",
       "  0.4961184561252594,\n",
       "  0.5120351314544678,\n",
       "  0.5149560570716858,\n",
       "  0.5112334489822388,\n",
       "  0.5055059194564819,\n",
       "  0.5079037547111511,\n",
       "  0.5235245227813721,\n",
       "  0.5012459754943848,\n",
       "  0.5081783533096313,\n",
       "  0.4989807605743408,\n",
       "  0.5098909735679626,\n",
       "  0.5147548913955688,\n",
       "  0.5018390417098999,\n",
       "  0.5036760568618774,\n",
       "  0.5208028554916382,\n",
       "  0.5051497220993042,\n",
       "  0.5100955963134766,\n",
       "  0.5150805711746216,\n",
       "  0.5110211372375488,\n",
       "  0.5110419392585754,\n",
       "  0.5125150084495544,\n",
       "  0.49870017170906067,\n",
       "  0.51557457447052,\n",
       "  0.5143122673034668,\n",
       "  0.5086447596549988,\n",
       "  0.5162914991378784,\n",
       "  0.5089075565338135,\n",
       "  0.5313922762870789,\n",
       "  0.5135542154312134],\n",
       " 'val_accuracy': [0.6699386239051819,\n",
       "  0.6803681254386902,\n",
       "  0.6987730264663696,\n",
       "  0.7276073694229126,\n",
       "  0.7478527426719666,\n",
       "  0.7570552229881287,\n",
       "  0.7582821846008301,\n",
       "  0.7496932744979858,\n",
       "  0.7496932744979858,\n",
       "  0.7588956952095032,\n",
       "  0.745398759841919,\n",
       "  0.7533742189407349,\n",
       "  0.7576687335968018,\n",
       "  0.7570552229881287,\n",
       "  0.762576699256897,\n",
       "  0.7613496780395508,\n",
       "  0.7674846649169922,\n",
       "  0.7644171714782715,\n",
       "  0.7687116861343384,\n",
       "  0.7717791199684143,\n",
       "  0.7644171714782715,\n",
       "  0.7656441926956177,\n",
       "  0.762576699256897,\n",
       "  0.7656441926956177,\n",
       "  0.7699386477470398,\n",
       "  0.7766871452331543,\n",
       "  0.7668711543083191,\n",
       "  0.7668711543083191,\n",
       "  0.7680981755256653,\n",
       "  0.766257643699646,\n",
       "  0.7638036608695984,\n",
       "  0.7736196517944336,\n",
       "  0.7773005962371826,\n",
       "  0.771165668964386,\n",
       "  0.7668711543083191,\n",
       "  0.7631902098655701,\n",
       "  0.774846613407135,\n",
       "  0.7644171714782715,\n",
       "  0.7717791199684143,\n",
       "  0.7668711543083191,\n",
       "  0.7674846649169922,\n",
       "  0.7644171714782715,\n",
       "  0.7668711543083191,\n",
       "  0.7631902098655701,\n",
       "  0.7619631886482239,\n",
       "  0.7742331027984619,\n",
       "  0.7631902098655701,\n",
       "  0.7644171714782715,\n",
       "  0.766257643699646,\n",
       "  0.7736196517944336,\n",
       "  0.7730061411857605,\n",
       "  0.7705521583557129,\n",
       "  0.7680981755256653,\n",
       "  0.7705521583557129,\n",
       "  0.7619631886482239,\n",
       "  0.7680981755256653,\n",
       "  0.7717791199684143,\n",
       "  0.7699386477470398,\n",
       "  0.7680981755256653,\n",
       "  0.774846613407135,\n",
       "  0.771165668964386,\n",
       "  0.7705521583557129,\n",
       "  0.7631902098655701,\n",
       "  0.7644171714782715,\n",
       "  0.7638036608695984,\n",
       "  0.771165668964386,\n",
       "  0.7717791199684143,\n",
       "  0.7723926305770874,\n",
       "  0.774846613407135,\n",
       "  0.7742331027984619,\n",
       "  0.7687116861343384,\n",
       "  0.7766871452331543,\n",
       "  0.7717791199684143,\n",
       "  0.771165668964386,\n",
       "  0.7693251371383667,\n",
       "  0.7742331027984619,\n",
       "  0.7650306820869446,\n",
       "  0.7607361674308777,\n",
       "  0.7742331027984619,\n",
       "  0.7742331027984619,\n",
       "  0.7852760553359985,\n",
       "  0.7680981755256653,\n",
       "  0.7760736346244812,\n",
       "  0.7742331027984619,\n",
       "  0.7588956952095032,\n",
       "  0.7705521583557129,\n",
       "  0.7705521583557129,\n",
       "  0.7730061411857605,\n",
       "  0.7785276174545288,\n",
       "  0.7809816002845764,\n",
       "  0.7809816002845764,\n",
       "  0.7687116861343384,\n",
       "  0.7730061411857605,\n",
       "  0.771165668964386,\n",
       "  0.7717791199684143,\n",
       "  0.7742331027984619,\n",
       "  0.7742331027984619,\n",
       "  0.7723926305770874,\n",
       "  0.7785276174545288,\n",
       "  0.7754601240158081,\n",
       "  0.7717791199684143,\n",
       "  0.7717791199684143,\n",
       "  0.7815951108932495,\n",
       "  0.7773005962371826,\n",
       "  0.7871165871620178,\n",
       "  0.7785276174545288,\n",
       "  0.7742331027984619,\n",
       "  0.7766871452331543,\n",
       "  0.7803680896759033,\n",
       "  0.7828220725059509,\n",
       "  0.7815951108932495,\n",
       "  0.7766871452331543,\n",
       "  0.7705521583557129,\n",
       "  0.7822085618972778,\n",
       "  0.7693251371383667,\n",
       "  0.7840490937232971,\n",
       "  0.7785276174545288,\n",
       "  0.7797545790672302,\n",
       "  0.792024552822113,\n",
       "  0.7736196517944336,\n",
       "  0.7779141068458557,\n",
       "  0.7766871452331543,\n",
       "  0.783435583114624,\n",
       "  0.7815951108932495,\n",
       "  0.7871165871620178,\n",
       "  0.783435583114624,\n",
       "  0.7815951108932495,\n",
       "  0.7889570593833923,\n",
       "  0.7871165871620178,\n",
       "  0.7871165871620178,\n",
       "  0.7895705699920654,\n",
       "  0.7883435487747192,\n",
       "  0.774846613407135,\n",
       "  0.7871165871620178,\n",
       "  0.7828220725059509,\n",
       "  0.7858895659446716,\n",
       "  0.7803680896759033,\n",
       "  0.7883435487747192,\n",
       "  0.7852760553359985,\n",
       "  0.783435583114624,\n",
       "  0.7901840209960938,\n",
       "  0.7797545790672302,\n",
       "  0.7846626043319702,\n",
       "  0.7895705699920654,\n",
       "  0.774846613407135,\n",
       "  0.7858895659446716,\n",
       "  0.7938650250434875,\n",
       "  0.783435583114624,\n",
       "  0.783435583114624,\n",
       "  0.7932515144348145,\n",
       "  0.7944785356521606,\n",
       "  0.7969325184822083,\n",
       "  0.7901840209960938,\n",
       "  0.783435583114624,\n",
       "  0.7907975316047668,\n",
       "  0.7889570593833923,\n",
       "  0.7785276174545288,\n",
       "  0.7815951108932495,\n",
       "  0.7901840209960938,\n",
       "  0.7766871452331543,\n",
       "  0.7926380634307861,\n",
       "  0.7963190078735352,\n",
       "  0.7840490937232971,\n",
       "  0.7858895659446716,\n",
       "  0.7963190078735352,\n",
       "  0.7907975316047668,\n",
       "  0.7846626043319702,\n",
       "  0.792024552822113,\n",
       "  0.7932515144348145,\n",
       "  0.7889570593833923,\n",
       "  0.7883435487747192,\n",
       "  0.7926380634307861,\n",
       "  0.7938650250434875,\n",
       "  0.800000011920929,\n",
       "  0.7963190078735352,\n",
       "  0.800613522529602,\n",
       "  0.7981594800949097,\n",
       "  0.8042944669723511,\n",
       "  0.7944785356521606,\n",
       "  0.7987729907035828,\n",
       "  0.7901840209960938,\n",
       "  0.8018404841423035,\n",
       "  0.7981594800949097,\n",
       "  0.7957054972648621,\n",
       "  0.7993865013122559,\n",
       "  0.7963190078735352,\n",
       "  0.7950920462608337,\n",
       "  0.7975460290908813,\n",
       "  0.800000011920929,\n",
       "  0.7950920462608337,\n",
       "  0.7993865013122559,\n",
       "  0.800000011920929,\n",
       "  0.8030675053596497,\n",
       "  0.7981594800949097,\n",
       "  0.8024539947509766,\n",
       "  0.800000011920929,\n",
       "  0.8049079775810242,\n",
       "  0.8012269735336304,\n",
       "  0.7865030765533447,\n",
       "  0.8055214881896973,\n",
       "  0.8012269735336304,\n",
       "  0.800000011920929,\n",
       "  0.8079754710197449,\n",
       "  0.7975460290908813,\n",
       "  0.8116564154624939,\n",
       "  0.8018404841423035,\n",
       "  0.8049079775810242,\n",
       "  0.7950920462608337,\n",
       "  0.8067484498023987,\n",
       "  0.8055214881896973,\n",
       "  0.8061349987983704,\n",
       "  0.8030675053596497,\n",
       "  0.800613522529602,\n",
       "  0.7963190078735352,\n",
       "  0.8049079775810242,\n",
       "  0.7969325184822083,\n",
       "  0.800000011920929,\n",
       "  0.7987729907035828,\n",
       "  0.8098159432411194,\n",
       "  0.8073619604110718,\n",
       "  0.8134969472885132,\n",
       "  0.7981594800949097,\n",
       "  0.7963190078735352,\n",
       "  0.8030675053596497,\n",
       "  0.8092024326324463,\n",
       "  0.8061349987983704,\n",
       "  0.8042944669723511,\n",
       "  0.8012269735336304,\n",
       "  0.8092024326324463,\n",
       "  0.8012269735336304,\n",
       "  0.8024539947509766,\n",
       "  0.8024539947509766,\n",
       "  0.800000011920929,\n",
       "  0.800000011920929,\n",
       "  0.8049079775810242,\n",
       "  0.792024552822113,\n",
       "  0.800613522529602,\n",
       "  0.8049079775810242,\n",
       "  0.800613522529602,\n",
       "  0.800000011920929,\n",
       "  0.7963190078735352,\n",
       "  0.8024539947509766,\n",
       "  0.7926380634307861,\n",
       "  0.7950920462608337,\n",
       "  0.8018404841423035,\n",
       "  0.7981594800949097,\n",
       "  0.8073619604110718,\n",
       "  0.808588981628418,\n",
       "  0.8147239089012146,\n",
       "  0.8030675053596497,\n",
       "  0.8042944669723511,\n",
       "  0.803680956363678,\n",
       "  0.803680956363678,\n",
       "  0.8042944669723511,\n",
       "  0.803680956363678,\n",
       "  0.8116564154624939,\n",
       "  0.8049079775810242,\n",
       "  0.803680956363678,\n",
       "  0.8024539947509766,\n",
       "  0.8110429644584656,\n",
       "  0.812269926071167,\n",
       "  0.7987729907035828,\n",
       "  0.8092024326324463,\n",
       "  0.8073619604110718,\n",
       "  0.8073619604110718,\n",
       "  0.803680956363678,\n",
       "  0.808588981628418,\n",
       "  0.8104294538497925,\n",
       "  0.8055214881896973,\n",
       "  0.8104294538497925,\n",
       "  0.8092024326324463,\n",
       "  0.8030675053596497,\n",
       "  0.8061349987983704,\n",
       "  0.8042944669723511,\n",
       "  0.8049079775810242,\n",
       "  0.8055214881896973,\n",
       "  0.808588981628418,\n",
       "  0.8098159432411194,\n",
       "  0.8042944669723511,\n",
       "  0.803680956363678,\n",
       "  0.8073619604110718,\n",
       "  0.8061349987983704,\n",
       "  0.8061349987983704,\n",
       "  0.7957054972648621,\n",
       "  0.8030675053596497,\n",
       "  0.8104294538497925,\n",
       "  0.8092024326324463,\n",
       "  0.8110429644584656,\n",
       "  0.8116564154624939,\n",
       "  0.8061349987983704,\n",
       "  0.8092024326324463,\n",
       "  0.8147239089012146,\n",
       "  0.8042944669723511,\n",
       "  0.8073619604110718,\n",
       "  0.808588981628418,\n",
       "  0.7987729907035828,\n",
       "  0.8153374195098877,\n",
       "  0.8104294538497925,\n",
       "  0.8104294538497925,\n",
       "  0.8153374195098877,\n",
       "  0.8024539947509766,\n",
       "  0.8049079775810242,\n",
       "  0.8055214881896973,\n",
       "  0.8141104578971863,\n",
       "  0.8061349987983704,\n",
       "  0.8116564154624939,\n",
       "  0.8098159432411194,\n",
       "  0.8171778917312622,\n",
       "  0.7993865013122559,\n",
       "  0.8159509301185608,\n",
       "  0.8079754710197449,\n",
       "  0.8147239089012146,\n",
       "  0.808588981628418,\n",
       "  0.8061349987983704,\n",
       "  0.8104294538497925,\n",
       "  0.8030675053596497,\n",
       "  0.8104294538497925,\n",
       "  0.8067484498023987,\n",
       "  0.8202453851699829,\n",
       "  0.8116564154624939,\n",
       "  0.8110429644584656,\n",
       "  0.8165644407272339,\n",
       "  0.8042944669723511,\n",
       "  0.8128834366798401,\n",
       "  0.8098159432411194,\n",
       "  0.8141104578971863,\n",
       "  0.812269926071167,\n",
       "  0.8134969472885132,\n",
       "  0.812269926071167,\n",
       "  0.8110429644584656,\n",
       "  0.8128834366798401,\n",
       "  0.808588981628418,\n",
       "  0.8184049129486084,\n",
       "  0.8147239089012146,\n",
       "  0.8110429644584656,\n",
       "  0.820858895778656,\n",
       "  0.820858895778656,\n",
       "  0.8202453851699829,\n",
       "  0.8177914023399353,\n",
       "  0.8184049129486084,\n",
       "  0.8147239089012146,\n",
       "  0.8165644407272339,\n",
       "  0.8079754710197449,\n",
       "  0.8128834366798401,\n",
       "  0.8049079775810242,\n",
       "  0.8092024326324463,\n",
       "  0.8177914023399353,\n",
       "  0.8184049129486084,\n",
       "  0.8165644407272339,\n",
       "  0.8067484498023987,\n",
       "  0.8159509301185608,\n",
       "  0.8171778917312622,\n",
       "  0.8196318745613098,\n",
       "  0.8147239089012146,\n",
       "  0.8153374195098877,\n",
       "  0.8018404841423035,\n",
       "  0.8177914023399353,\n",
       "  0.8147239089012146,\n",
       "  0.8159509301185608,\n",
       "  0.8165644407272339,\n",
       "  0.8134969472885132,\n",
       "  0.8049079775810242,\n",
       "  0.8061349987983704,\n",
       "  0.8042944669723511,\n",
       "  0.8214724063873291,\n",
       "  0.8134969472885132,\n",
       "  0.8110429644584656,\n",
       "  0.8196318745613098,\n",
       "  0.8202453851699829,\n",
       "  0.8171778917312622,\n",
       "  0.8134969472885132,\n",
       "  0.8055214881896973,\n",
       "  0.8092024326324463,\n",
       "  0.8184049129486084,\n",
       "  0.8098159432411194,\n",
       "  0.8159509301185608,\n",
       "  0.8116564154624939,\n",
       "  0.820858895778656,\n",
       "  0.8110429644584656,\n",
       "  0.8190184235572815,\n",
       "  0.8306748270988464,\n",
       "  0.8184049129486084,\n",
       "  0.820858895778656,\n",
       "  0.8134969472885132,\n",
       "  0.8190184235572815,\n",
       "  0.8134969472885132,\n",
       "  0.8300613760948181,\n",
       "  0.8233128786087036,\n",
       "  0.8153374195098877,\n",
       "  0.8184049129486084,\n",
       "  0.8098159432411194,\n",
       "  0.8177914023399353,\n",
       "  0.820858895778656,\n",
       "  0.8134969472885132,\n",
       "  0.8171778917312622,\n",
       "  0.8220859169960022,\n",
       "  0.8171778917312622,\n",
       "  0.8141104578971863,\n",
       "  0.8190184235572815,\n",
       "  0.8226993680000305,\n",
       "  0.8165644407272339,\n",
       "  0.820858895778656,\n",
       "  0.8153374195098877,\n",
       "  0.8134969472885132,\n",
       "  0.8190184235572815,\n",
       "  0.8214724063873291,\n",
       "  0.8159509301185608,\n",
       "  0.8177914023399353,\n",
       "  0.8171778917312622,\n",
       "  0.8196318745613098,\n",
       "  0.820858895778656,\n",
       "  0.8233128786087036,\n",
       "  0.8220859169960022,\n",
       "  0.8251533508300781,\n",
       "  0.8202453851699829,\n",
       "  0.8196318745613098,\n",
       "  0.8220859169960022,\n",
       "  0.8251533508300781,\n",
       "  0.8128834366798401,\n",
       "  0.8165644407272339,\n",
       "  0.8325153589248657,\n",
       "  0.8214724063873291,\n",
       "  0.8282208442687988,\n",
       "  0.8276073336601257,\n",
       "  0.8196318745613098,\n",
       "  0.8245398998260498,\n",
       "  0.8190184235572815,\n",
       "  0.8153374195098877,\n",
       "  0.820858895778656,\n",
       "  0.8257668614387512,\n",
       "  0.8251533508300781,\n",
       "  0.8202453851699829,\n",
       "  0.8300613760948181,\n",
       "  0.8257668614387512,\n",
       "  0.8263803720474243,\n",
       "  0.8202453851699829,\n",
       "  0.820858895778656,\n",
       "  0.8165644407272339,\n",
       "  0.8263803720474243,\n",
       "  0.8226993680000305,\n",
       "  0.8190184235572815,\n",
       "  0.8245398998260498,\n",
       "  0.820858895778656,\n",
       "  0.8245398998260498,\n",
       "  0.8245398998260498,\n",
       "  0.8196318745613098,\n",
       "  0.8239263892173767,\n",
       "  0.8233128786087036,\n",
       "  0.8177914023399353,\n",
       "  0.8263803720474243,\n",
       "  0.8263803720474243,\n",
       "  0.833128809928894,\n",
       "  0.8233128786087036,\n",
       "  0.8288343548774719,\n",
       "  0.8245398998260498,\n",
       "  0.8245398998260498,\n",
       "  0.8251533508300781,\n",
       "  0.8276073336601257,\n",
       "  0.8300613760948181,\n",
       "  0.8312883377075195,\n",
       "  0.8184049129486084,\n",
       "  0.829447865486145,\n",
       "  0.8276073336601257,\n",
       "  0.8300613760948181,\n",
       "  0.8239263892173767,\n",
       "  0.8245398998260498,\n",
       "  0.8319018483161926,\n",
       "  0.8325153589248657,\n",
       "  0.820858895778656,\n",
       "  0.8269938826560974,\n",
       "  0.8251533508300781,\n",
       "  0.8239263892173767,\n",
       "  0.8276073336601257,\n",
       "  0.8312883377075195,\n",
       "  0.8355827927589417,\n",
       "  0.8312883377075195,\n",
       "  0.8282208442687988,\n",
       "  0.8245398998260498,\n",
       "  0.8239263892173767,\n",
       "  0.8202453851699829,\n",
       "  0.8245398998260498,\n",
       "  0.8226993680000305,\n",
       "  0.8312883377075195,\n",
       "  0.8374233245849609,\n",
       "  0.829447865486145,\n",
       "  0.8202453851699829,\n",
       "  0.829447865486145,\n",
       "  0.8312883377075195,\n",
       "  0.8202453851699829,\n",
       "  0.8276073336601257,\n",
       "  0.8361963033676147,\n",
       "  0.8306748270988464,\n",
       "  0.8300613760948181,\n",
       "  0.8325153589248657,\n",
       "  0.8263803720474243,\n",
       "  0.8251533508300781,\n",
       "  0.8239263892173767,\n",
       "  0.8269938826560974,\n",
       "  0.8177914023399353,\n",
       "  0.8269938826560974]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               4096      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 70,402\n",
      "Trainable params: 70,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./cmLapsed_weights.txt', 'w')  # 参数提取\n",
    "for v in model.trainable_variables:\n",
    "    file.write(str(v.name) + '\\n')\n",
    "    file.write(str(v.shape) + '\\n')\n",
    "    file.write(str(v.numpy()) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 - 0s - loss: 1.1573 - accuracy: 0.7370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1573482751846313, 0.7369999885559082]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(df):\n",
    "    \"\"\"\n",
    "    PS : 要求传入的df是连续性变量\n",
    "    \n",
    "    Applies function along input axis(default 0) of DataFrame\n",
    "    标准化方法有多种：\n",
    "    归一化Max-Min\n",
    "    实现中心化Z-Score\n",
    "    用于稀疏数据的MaxAbs\n",
    "    针对离群点的RobustScaler\n",
    "    \n",
    "    \"\"\"\n",
    "    return df.apply(lambda column: (column - column.mean()) / column.std())#特征缩放 z-score 标准化(zero-mean normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2-gpu",
   "language": "python",
   "name": "tensorflow2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
